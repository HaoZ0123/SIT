{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Advertising Analytics - Click Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lulu Zhu 17FALL BIA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advertising teams want to analyze their immense stores and varieties of data requiring a scalable, extensible, and elastic platform. Advanced analytics, including but not limited to classification, clustering, recognition, prediction, and recommendations allow these organizations to gain deeper insights from their data and drive business outcomes. As data of various types grow in volume, Apache Spark provides an API and distributed compute engine to process data easily and in parallel, thereby decreasing time to value. \n",
    "\n",
    "Let’s look at a concrete example with the Click-Through Rate Prediction dataset of ad impressions and clicks from the data science website Kaggle. The goal of this workflow is to create a machine learning model that predicts whether or not there will be a click.\n",
    "\n",
    "To build our advanced analytics workflow, let’s focus on the three main steps:   \n",
    "- Building the ETL process  \n",
    "- Exploratory Data Analysis   \n",
    "- Advanced Analytics / Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Source:** https://www.kaggle.com/c/avazu-ctr-prediction/data  \n",
    "**Note:** This notebook is just a simplified instruction on pySpark. If you want to learn more, please refer to the official document.   \n",
    "        Also, we will only use sample data in this notebook for less processing time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BUILDING THE ETL PROCESS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a Spark DataFrame – an immutable, tabular, distributed data structure on Spark cluster.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: decimal(20,0) (nullable = true)\n",
      " |-- click: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- C1: integer (nullable = true)\n",
      " |-- banner_pos: integer (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- site_domain: string (nullable = true)\n",
      " |-- site_category: string (nullable = true)\n",
      " |-- app_id: string (nullable = true)\n",
      " |-- app_domain: string (nullable = true)\n",
      " |-- app_category: string (nullable = true)\n",
      " |-- device_id: string (nullable = true)\n",
      " |-- device_ip: string (nullable = true)\n",
      " |-- device_model: string (nullable = true)\n",
      " |-- device_type: integer (nullable = true)\n",
      " |-- device_conn_type: integer (nullable = true)\n",
      " |-- C14: integer (nullable = true)\n",
      " |-- C15: integer (nullable = true)\n",
      " |-- C16: integer (nullable = true)\n",
      " |-- C17: integer (nullable = true)\n",
      " |-- C18: integer (nullable = true)\n",
      " |-- C19: integer (nullable = true)\n",
      " |-- C20: integer (nullable = true)\n",
      " |-- C21: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# start a spark session \n",
    "spark = SparkSession.builder.master(\"local\").appName(\"CTR Models\").config(\n",
    "    \"spark.executor.memory\", \"8g\").config(\n",
    "    \"spark.driver.memory\", \"15g\").getOrCreate()\n",
    "\n",
    "# load data with inferred schema \n",
    "df = spark.read.load(\"./sample_data.csv\", \n",
    "                     format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# The inferred schema can be seen using .printSchema().\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORATORY DATA ANALYSIS\n",
    "\n",
    "In this section, we will use two methods to explore the datafame:   \n",
    "- Build-in methods from spark dataframe. Check all the methods from https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame\n",
    "- Querying data using SQL  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using Spark DataFrame Methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|click|\n",
      "+-----+\n",
      "|    0|\n",
      "|    1|\n",
      "|    0|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select columns \n",
    "df.select(\"click\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count only unique values in one column \n",
    "df.select(\"click\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|click|count|\n",
      "+-----+-----+\n",
      "|    1| 6905|\n",
      "|    0|33348|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# count the occurance of each unique value \n",
    "df.groupBy(\"click\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|summary|  app_id|\n",
      "+-------+--------+\n",
      "|  count|   40253|\n",
      "|   mean|Infinity|\n",
      "| stddev|     NaN|\n",
      "|    min|000d6291|\n",
      "|    max|ffc6ffd0|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get statistics by 'describe'\n",
    "df.describe([\"app_id\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----+-----+-----+\n",
      "|summary|  C14|  C15|  C16|  C17|\n",
      "+-------+-----+-----+-----+-----+\n",
      "|  count|40253|40253|40253|40253|\n",
      "|    min|  375|  120|   20|  112|\n",
      "|    25%|16920|  320|   50| 1863|\n",
      "|    max|24041| 1024| 1024| 2756|\n",
      "+-------+-----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get statistics by 'summary' after selection \n",
    "# Available statistics are: - count - mean - stddev - min - max \n",
    "                          # - arbitrary approximate percentiles specified as a percentage (eg, 75%)\n",
    "df.select(\"C14\",\"C15\",\"C16\",\"C17\").summary(\"count\", \"min\",\"25%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n",
      "|                  id|click|    hour|  C1|banner_pos| site_id|site_domain|site_category|  app_id|app_domain|app_category|device_id|device_ip|device_model|device_type|device_conn_type|  C14|C15|C16| C17|C18|C19|   C20|C21|\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n",
      "|10114646856300029525|    1|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 093fa194|    5096d134|          1|               0|15701|320| 50|1722|  0| 35|100084| 79|\n",
      "|10313349640527177600|    1|14102100|1005|         0|5b08c53b|   7687a86e|     3e814130|ecad2386|  7801e8d9|    07d7df22| a99f214a| 302f4cc4|    c6263d8a|          1|               0|17653|300|250|1994|  2| 39|    -1| 33|\n",
      "|10531475579981335532|    1|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 47f968fc|    04f5b394|          1|               0|15707|320| 50|1722|  0| 35|    -1| 79|\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select rows by condition, where() is an alias for filter().\n",
    "df.filter(df.click == 1).show(3)\n",
    "# df.where(df.click == 1).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using Spark SQL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|banner_pos|                CTR|\n",
      "+----------+-------------------+\n",
      "|         0|0.16690831262082298|\n",
      "|         1|0.18319657509810917|\n",
      "|         2|0.16666666666666667|\n",
      "|         3|              0E-17|\n",
      "|         4|0.20000000000000000|\n",
      "|         5|              0E-17|\n",
      "|         7|0.26666666666666667|\n",
      "+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a temperate view: createTempView vs createOrReplaceTempView\n",
    "df.createTempView(\"test\")\n",
    "\n",
    "# calculates the click through rate (CTR) by banner position\n",
    "spark.sql(\"select banner_pos,\\\n",
    "          sum(case when click = 1 then 1 else 0 end) / (count(1) * 1.0) as CTR from test\\\n",
    "          group by 1\\\n",
    "          order by 1\").show()\n",
    "\n",
    "# drop the temperate view \n",
    "spark.catalog.dropTempView(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREDICT THE CLICKS\n",
    "\n",
    "Three common procedures in Machine Learning will be done with Spark in this session.\n",
    "- Data Enrichment \n",
    "- Feature Engineering Pipline \n",
    "- Machine Learning Model & Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Enrichment  - Date\n",
    "We can always generate some new columns from existing columns. Sometimes, those derived columns may be more relevant to the target than their original columns. Therefore, in this part, we will first generate \"date\" column from original \"hour\" column. The \"date\" column is in datetime type. Then new features such as day_of_week and hour_of_day will be derived from \"date\" column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import to_timestamp, date_format,hour\n",
    "\n",
    "# withColumn(): Returns a new DataFrame by adding a column or replacing the existing column that has the same name.\n",
    "\n",
    "# 1. Create a string column 'date' from 'hour'\n",
    "df = df.withColumn(\"date\", df[\"hour\"].cast(StringType()))\n",
    "\n",
    "# 2. Convert the 'date' column to time format \n",
    "df = df.withColumn(\"date\", to_timestamp(\"date\", \"yyMMddHH\"))\n",
    "\n",
    "# 3. Create a new column 'day_of_week' from 'date'\n",
    "df = df.withColumn(\"day_of_week\", date_format(\"date\", \"E\"))\n",
    "\n",
    "# 4. Create a new column 'hour_of_day' from 'date'\n",
    "df = df.withColumn(\"hour_of_day\", hour(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+-------------------+-----------+-----------+\n",
      "|                  id|click|    hour|  C1|banner_pos| site_id|site_domain|site_category|  app_id|app_domain|app_category|device_id|device_ip|device_model|device_type|device_conn_type|  C14|C15|C16| C17|C18|C19|   C20|C21|               date|day_of_week|hour_of_day|\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+-------------------+-----------+-----------+\n",
      "|10092095701542991716|    0|14102100|1005|         0|85f751fd|   c4e18dd6|     50e219e0|f888bf4c|  5b9c592b|    0f2161f8| 33d33eb8| 15fb30ea|    be6db1d7|          1|               0|18987|320| 50|2158|  3|291|100190| 61|2014-10-21 00:00:00|        Tue|          0|\n",
      "|10114646856300029525|    1|14102100|1005|         0|1fbe01fe|   f3845767|     28905ebd|ecad2386|  7801e8d9|    07d7df22| a99f214a| 093fa194|    5096d134|          1|               0|15701|320| 50|1722|  0| 35|100084| 79|2014-10-21 00:00:00|        Tue|          0|\n",
      "+--------------------+-----+--------+----+----------+--------+-----------+-------------+--------+----------+------------+---------+---------+------------+-----------+----------------+-----+---+---+----+---+---+------+---+-------------------+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.  Feature Engineering Pipline \n",
    "Once we have familiarized ourselves with our data, we can proceed to the machine learning phase, where we convert our data into features for input to a machine learning algorithm and produce a trained model with which we can predict. Because Spark ML algorithms take **a column of feature vectors of doubles as input**, a typical feature engineering workflow includes:  \n",
    "- Identifying numeric and categorical features   \n",
    "- String indexing  \n",
    "- Assembling them all into a sparse vector  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Identifying numeric and categorical features with less than 70 categories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique number of id (decimal(20,0)): 40253\n",
      "unique number of click (int): 2\n",
      "unique number of hour (int): 240\n",
      "unique number of C1 (int): 7\n",
      "unique number of banner_pos (int): 7\n",
      "unique number of site_id (string): 1054\n",
      "unique number of site_domain (string): 904\n",
      "unique number of site_category (string): 18\n",
      "unique number of app_id (string): 877\n",
      "unique number of app_domain (string): 68\n",
      "unique number of app_category (string): 20\n",
      "unique number of device_id (string): 6858\n",
      "unique number of device_ip (string): 33788\n",
      "unique number of device_model (string): 2378\n",
      "unique number of device_type (int): 4\n",
      "unique number of device_conn_type (int): 4\n",
      "unique number of C14 (int): 1445\n",
      "unique number of C15 (int): 7\n",
      "unique number of C16 (int): 8\n",
      "unique number of C17 (int): 380\n",
      "unique number of C18 (int): 4\n",
      "unique number of C19 (int): 63\n",
      "unique number of C20 (int): 149\n",
      "unique number of C21 (int): 59\n",
      "unique number of date (timestamp): 240\n",
      "unique number of day_of_week (string): 7\n",
      "unique number of hour_of_day (int): 24\n"
     ]
    }
   ],
   "source": [
    "maxBins = 70\n",
    "str_featues = []\n",
    "num_features = []\n",
    "\n",
    "for feature in df.dtypes:\n",
    "    \n",
    "    # print unique value and data type for each feature\n",
    "    feature_name = str(feature[0])\n",
    "    feature_type = str(feature[1])\n",
    "    uni = df.select(feature_name).distinct().count()\n",
    "    print(\"unique number of {} ({}):\".format(feature_name, feature_type),uni)\n",
    "    \n",
    "    # find out features with integer type\n",
    "    if feature_type == \"int\" or feature_type == \"decimal(20,0)\":\n",
    "        \n",
    "        num_features.append(feature_name)\n",
    "        \n",
    "    # find out string feature and it's unique value less than maxBins\n",
    "    elif feature_type == \"string\" and uni < maxBins:\n",
    "        \n",
    "        str_featues.append(feature_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is in total 16 numerical features:['id', 'click', 'hour', 'C1', 'banner_pos', 'device_type', 'device_conn_type', 'C14', 'C15', 'C16', 'C17', 'C18', 'C19', 'C20', 'C21', 'hour_of_day']\n",
      "There is in total 4 string features:['site_category', 'app_domain', 'app_category', 'day_of_week']\n"
     ]
    }
   ],
   "source": [
    "print(\"There is in total {} numerical features:{}\".format(len(num_features),num_features)) \n",
    "print(\"There is in total {} string features:{}\".format(len(str_featues),str_featues)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove id and label from features \n",
    "num_features.remove(\"id\")\n",
    "num_features.remove(\"click\")\n",
    "num_features.remove(\"hour\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 String indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "StringIndexer encodes a string column of labels to a column of label indices. The indices are in [0, numLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, ChiSqSelector\n",
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\") for column in str_featues]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stringindexer vs One-Hot-Encoder**\n",
    "\n",
    "You may have noticed that while we use string indexer but we are not applying One Hot Encoder (OHE). When using StringIndexer, categorical features are kept as k-ary categorical features. A tree node will test if feature X has a value in {subset of categories}. With OHE: Your categorical features are turned into a bunch of binary features. A tree node will test if feature X = category a vs. all the other categories (one vs. rest test).  \n",
    "\n",
    "When using only StringIndexer, the benefits include:  \n",
    "• There are fewer features to choose  \n",
    "• Each node’s test is more expressive than with binary 1-vs-rest features  \n",
    "\n",
    "Therefore, for because for tree based methods, it is preferable to not use OHE as it is a less expressive test and it takes up more space. But for non-tree- based algorithms such as like linear regression, you must use OHE or else the model will impose a false and misleading ordering on categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Assembling them all into a sparse vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. \n",
    "\n",
    "VectorAssembler accepts the following input column types: all numeric types, boolean type, and vector type. In each row, the values of the input columns will be concatenated into a vector in the specified order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_names = [\"{}_index\".format(x) for x in str_featues] + num_features\n",
    "\n",
    "assembler = VectorAssembler(inputCols= index_names, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Building the Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=indexers+[assembler])\n",
    "\n",
    "data = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=Decimal('10092095701542991716'), click=0, hour=14102100, C1=1005, banner_pos=0, site_id='85f751fd', site_domain='c4e18dd6', site_category='50e219e0', app_id='f888bf4c', app_domain='5b9c592b', app_category='0f2161f8', device_id='33d33eb8', device_ip='15fb30ea', device_model='be6db1d7', device_type=1, device_conn_type=0, C14=18987, C15=320, C16=50, C17=2158, C18=3, C19=291, C20=100190, C21=61, date=datetime.datetime(2014, 10, 21, 0, 0), day_of_week='Tue', hour_of_day=0, site_category_index=0.0, app_domain_index=13.0, app_category_index=1.0, day_of_week_index=0.0, features=DenseVector([0.0, 13.0, 1.0, 0.0, 1005.0, 0.0, 1.0, 0.0, 18987.0, 320.0, 50.0, 2158.0, 3.0, 291.0, 100190.0, 61.0, 0.0]))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 2., 0., 0.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DenseVector VS SparseVector\n",
    "from  pyspark.mllib.linalg import SparseVector \n",
    "a = SparseVector(5, {0: 1.0, 2: 2.0})\n",
    "a.toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Modeling - GBT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Spliting train and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select feature col \n",
    "input_data = data.select([\"features\",\"click\"])\n",
    "\n",
    "# split dataset into train set and test set \n",
    "train_data, test_data = input_data.randomSplit([.8,.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 the GBT model \n",
    "\n",
    "Gradient boosting is a machine learning technique for classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# define the classifier \n",
    "gbt = GBTClassifier(labelCol=\"click\", featuresCol=\"features\", maxBins=maxBins)\n",
    "\n",
    "# train the classifier with train data \n",
    "GBT = gbt.fit(train_data)\n",
    "\n",
    "# predict test data \n",
    "predictions = GBT.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(17, {0: 0.0831, 1: 0.2354, 2: 0.0835, 3: 0.0791, 4: 0.0291, 5: 0.0126, 7: 0.0137, 8: 0.1002, 9: 0.0018, 10: 0.0165, 11: 0.0292, 12: 0.0529, 13: 0.0715, 14: 0.0386, 15: 0.0643, 16: 0.0884})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out some characteristics of the model \n",
    "GBT.featureImportances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GBT.getNumTrees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Choosing Evaluation method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# choose the auc score as evaluator \n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"click\", metricName='areaUnderROC')\n",
    "\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6752495349310862\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work\n",
    "**1. Unbalanced Data**: resampling...  \n",
    "**2. More Feature Engineering**: feature hashing...  \n",
    "**3. Feature Selection**: ChiSqSelector ...   \n",
    "**4. Parameter Optimization**: Grid Search ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference \n",
    "- Four Real-Life Machine Learning Use Cases, Databricks    \n",
    "- Apache Spark 2.4.1 https://spark.apache.org/docs/latest/api/python/index.html  \n",
    "- Gradient boosting https://en.wikipedia.org/wiki/Gradient_boosting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
