{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Web Scraping by API </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scrape data through APIs \n",
    "- Online content providers usually provide APIs for you to access data. Two types of APIs:\n",
    "   * Python packages: e.g. tweepy package from Twitter\n",
    "   * REST APIs: e.g. OMDB APIs (http://www.omdbapi.com), or TMDB (https://developers.themoviedb.org/3/getting-started)\n",
    "- You need to read documentation of APIs to figure out how to access data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scrape data by REST APIs (e.g. OMDB API)\n",
    "- A REST API is a web service that uses HTTP requests to GET, PUT, POST and DELETE data\n",
    "- Experiment:\n",
    "    - Get an API key here: http://www.omdbapi.com/apikey.aspx and follow the instruction to activate the key\n",
    "    - Use API, e.g. **http://www.omdbapi.com/<font color=\"blue\"><b>?</b></font>t=Rogue+One<font color=\"blue\"><b>&</b></font>plot=full<font color=\"blue\"><b>&</b></font>r=json<font color=\"blue\"><b>&</b></font>apikey={your api key}**, where\n",
    "        - **t=Rogue+One**: specify the movie title \n",
    "        - **plot=full**: return full plot\n",
    "        - **r=json**: result is in json format\n",
    "        - **apikey**: use your api key \n",
    "    - Note the format of URL:\n",
    "        - API endpoint: http://www.omdbapi.com/ \n",
    "        - parameters appear in the URL after the question mark (<font color=\"blue\"><b>?</b></font>) after the endpoint\n",
    "        - all parameters are concatenated by <font color=\"blue\"><b>\"&\"</b></font>  \n",
    "    - You can directly paste the above API to your browser\n",
    "    - Or issue API calls using requests\n",
    "- You need to read API documentation to understand how to specify parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.1. search movies by name\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "title='Rogue+One'\n",
    "\n",
    "# Search API: http://www.omdbapi.com/\n",
    "# has four parameters: title, full plot, result format, and api_key\n",
    "# For the get methods, parameters are attached to API URL after a \"?\"\n",
    "# Parameters are separated by \"&\"\n",
    "\n",
    "# to test, apply for an api key and use the key ere\n",
    "url=\"http://www.omdbapi.com/?t=\"+title+\\\n",
    "    \"&plot=full&r=json&apikey={your key here}\"\n",
    "\n",
    "# invoke the API \n",
    "r = requests.get(url)\n",
    "\n",
    "# if the API call returns a successful response\n",
    "if r.status_code==200:\n",
    "    \n",
    "    # This API call returns a json object\n",
    "    # r.json() gives the json object\n",
    "    result = r.json()\n",
    "    print (json.dumps(result, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.2.  Another way to pass parameters\n",
    "\n",
    "parameters = {'t': 'Rogue+One', \n",
    "              'plot': 'full',\n",
    "              'r': 'json',\n",
    "              'apikey':{your key here}}\n",
    "\n",
    "r=requests.get('http://www.omdbapi.com/', params=parameters)\n",
    "\n",
    "# in case authentication is needed, use\n",
    "# r = requests.get('https://api.github.com/user', auth=('user', 'pass'))\n",
    "\n",
    "# if the API call returns a successful response\n",
    "if r.status_code==200:\n",
    "    \n",
    "    # This API call returns a json object\n",
    "    # r.json() gives the json object\n",
    "    print (json.dumps(r.json(), indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. JSON (JavaScript Object Notation)\n",
    "\n",
    "### What is JSON\n",
    "- A lightweight data-interchange format\n",
    "- \"self-describing\" and easy to understand\n",
    "- the JSON format is text only \n",
    "- Language independent: can be read and used as a data format by any programming language\n",
    "\n",
    "###  JSON Syntax Rules\n",
    "JSON syntax is derived from JavaScript object notation syntax:\n",
    "- Data is in **name/value** pairs separated by commas\n",
    "- Curly braces hold objects\n",
    "- Square brackets hold arrays\n",
    "\n",
    "### A JSON object is:\n",
    "- **a dictionary** or \n",
    "- a **list of dictionaries**\n",
    "\n",
    "### Useful JSON functions\n",
    "- dumps: save json object to string\n",
    "- dump: save json object to file\n",
    "- loads: load from a string in json format\n",
    "- load: load from a file in json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.1 API returns a JSON object \n",
    "\n",
    "parameters = {'s': 'harry potter',# search by title\n",
    "              'plot': 'short', \n",
    "              'page': 1,# get 1st page\n",
    "              'r': 'json', # result format: json/xml\n",
    "              'apikey':{your key here}}\n",
    "\n",
    "r = requests.get('http://www.omdbapi.com/', params=parameters)\n",
    "if r.status_code==200:\n",
    "    result = r.json()\n",
    "    print(result)\n",
    "    \n",
    "# you only retrieve the first 10 entries\n",
    "# how to retrieve all results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3.2. Parse JSON object (a dictionary)\n",
    "\n",
    "# get a list of dictionaries\n",
    "movies = result [\"Search\"]\n",
    "\n",
    "# convert to string\n",
    "s = json.dumps(movies, indent=4)\n",
    "print(s)\n",
    "\n",
    "# load from a string\n",
    "movies1 = json.loads(s)\n",
    "print(movies1)\n",
    "\n",
    "# save to file\n",
    "json.dump(movies, open(\"movies.json\",\"w\"))\n",
    "\n",
    "# load from file\n",
    "movies1 = json.load(open(\"movies.json\",\"r\"))\n",
    "print(movies1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Collect Tweets\n",
    "\n",
    "- Real-time tweets: Tweepy package\n",
    "  -  Install package\n",
    "  -  Apply for authentication keys\n",
    "- Hostorical tweets \n",
    "  - You can always search tweets at https://twitter.com/search and then scrape the results returned\n",
    "  - Note that there is **no authentication needed**!\n",
    "  - For reference, check github project, https://github.com/Jefferson-Henrique/GetOldTweets-pythonyou \n",
    "  - Motivated by this project, let's try the following code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Search historical tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.1 Scrape past tweets using API \n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "\n",
    "# User agent must be defined in http request header\n",
    "# a user agent is software that is acting on behalf of \n",
    "# a user. Usually it tells the browser used.\n",
    "# some websites reject requests without a user agent\n",
    "headers = { 'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.'\n",
    "                              '86 Safari/537.36'}\n",
    "\n",
    "# specify parameters as a dictionary\n",
    "payload={\"f\":\"tweets\",  # retrieve tweets\n",
    "         \"q\":\"blockchain since:2017-09-10 until:2017-09-12\", # query string\n",
    "         \"max_position\":''} # max_position of results (paging purpose)\n",
    "\n",
    "# send a request with parameters and headers\n",
    "r=requests.get(\"https://twitter.com/i/search/timeline\",\\\n",
    "              params=payload, headers=headers)\n",
    "\n",
    "# this is equivalent to type the following URL in \n",
    "# https://twitter.com/search?&q='blockchain since:2017-09-10 until:2017-09-12'\n",
    "    \n",
    "if r.status_code==200:\n",
    "    result=r.json()\n",
    "    print(result)\n",
    "    \n",
    "    # get html source code of tweets\n",
    "    tweets_html = result['items_html']\n",
    "    \n",
    "    # Search returns tweets in the decending order of time\n",
    "    # retrieve the position of the earliest tweets returned\n",
    "    min_position = result['min_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4.1.2. define a function to parse tweets html \n",
    "# using BeautifulSoup\n",
    "\n",
    "def getTweets(tweets_html):\n",
    "    \n",
    "    result=[]\n",
    "    \n",
    "    soup=BeautifulSoup(tweets_html, \"html.parser\")\n",
    "\n",
    "    tweets=soup.select('div.js-stream-tweet')\n",
    "\n",
    "    for t in tweets:\n",
    "        username, text, timestamp, tweet_id = '','','',''\n",
    "        select_user = t.select(\"span.username.u-dir b\")\n",
    "        if select_user!=[]:\n",
    "            username=select_user[0].get_text()\n",
    "    \n",
    "        select_text = t.select(\"p.js-tweet-text\")\n",
    "        if select_text!=[]:\n",
    "            text=select_text[0].get_text()\n",
    "    \n",
    "        time_select = t.select(\"small.time span.js-short-timestamp\")\n",
    "        if time_select!=[]:\n",
    "            timestamp=int(time_select[0][\"data-time\"])\n",
    "            timestamp=datetime.fromtimestamp(timestamp).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "        tweet_id = t[\"data-tweet-id\"]\n",
    "    \n",
    "        #print(username, text, timestamp, tweet_id, \"\\n\")\n",
    "        \n",
    "        result.append({\"user\":username, \"text\":text, \"date\":timestamp})\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.3. Parse tweets using the function\n",
    "\n",
    "tweets=getTweets(tweets_html)\n",
    "print(\"total tweets:\", len(tweets))\n",
    "print(json.dumps(tweets, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.4. What if we want to return more?\n",
    "\n",
    "# Note that the search returns tweets \n",
    "# in the decending order of time\n",
    "# set the max_position to \n",
    "# the min_position of last search\n",
    "\n",
    "payload={\"f\":\"tweets\",  # retrieve tweets\n",
    "         \"q\":\"blockchain since:2017-09-10 until:2017-09-12\", # query string\n",
    "         \"max_position\":min_position} \n",
    "\n",
    "# search again\n",
    "r=requests.get(\"https://twitter.com/i/search/timeline\",\\\n",
    "              params=payload, headers=headers)\n",
    "\n",
    "if r.status_code==200:\n",
    "    result=r.json()\n",
    "    min_position = result['min_position']\n",
    "    tweets_html = result['items_html']\n",
    "    \n",
    "    tweets=getTweets(tweets_html)\n",
    "    print(\"total tweets:\", len(tweets))\n",
    "    print(json.dumps(tweets, indent=4))\n",
    "    \n",
    "# You can use a loop to keep sending requests\n",
    "# until all tweets satisfying your criteria\n",
    "# has been fetched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1.5. Generate Wordcloud\n",
    "    \n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# concate all tweets into one \n",
    "\n",
    "text = \" \".join([t[\"text\"] for t in tweets])\n",
    "#print(text)\n",
    "\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "\n",
    "wordcloud = WordCloud(max_font_size=60).generate(text);\n",
    "plt.figure();\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\");\n",
    "plt.axis(\"off\");\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 2. Access tweet stream (e.g. real-time tweets) through tweepy package\n",
    "\n",
    "- Please read\n",
    "https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/\n",
    "- You'll need to apply for authentication keys and tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
