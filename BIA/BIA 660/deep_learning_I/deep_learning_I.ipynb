{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Deep Learning and Text Analytics</center>\n",
    "\n",
    "References:\n",
    "- General introduction\n",
    "     - http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/\n",
    "- Word vector:\n",
    "     - https://code.google.com/archive/p/word2vec/\n",
    "- Keras tutorial\n",
    "     - https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "- CNN\n",
    "     - http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agenda\n",
    "- Introduction to neural networks\n",
    "- Word/Document Vectors (vector representation of words/phrases/paragraphs)\n",
    "- Convolutionary neural network (CNN)\n",
    "- Application of CNN in text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Introduction neural networks\n",
    "- A neural network is a computational model inspired by the way biological neural networks in the human brain process information.\n",
    "- Neural networks have been widely applied in speech recognition, computer vision and text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Single Neuron\n",
    "\n",
    "<img src=\"single_neuron.png\" width=\"60%\">\n",
    "$$h_{W,b}(x)=f(w_1x_1+w_2x_2+w_3x_3+b)$$\n",
    "- Basic components:\n",
    "    - **input** ($X$): $[x_1, x_2, x_3]$\n",
    "    - **weight** ($W$): $[w_1, w_2, w_3]$\n",
    "    - **bias**: $b$\n",
    "    - **activation** function: $f$\n",
    "- Different activation functions:\n",
    "    - **Sigmoid** (logistic function): takes a real-valued input and squashes it to range [0,1]. $$f(z)=\\frac{1}{1+e^{-z}}$$, where $z=w_1x_1+w_2x_2+w_3x_3+b$\n",
    "    - Tanh (hyperbolic tangent): takes a real-valued input and squashes it to the range [-1, 1]. $$f(z)=tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}$$\n",
    "    - ReLU (Rectified Linear Unit): $$f(z)=max(0,z)$$   \n",
    "    - **Softmax** (normalized exponential function): a generalization of the logistic function. If $z=[z_1, z_2, ..., z_k]$ is a $k$-dimensional vector, $$f(z)_{j \\in k}=\\frac{e^{z_j}}{\\sum_{i=1}^k{e^{z_i}}}$$ \n",
    "     - $f(z)_{j} \\in [0,1]$\n",
    "     - $\\sum_{j \\in k} {f(z)_{j}} =1 $\n",
    "     - $f(z)_{j}$ is treated as the **probability** of component $j$, a probability distribution over $k$ different possible outcomes\n",
    "     - e.g. in multi-label classification, softmax gives a probability of each label "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Neural Network Model\n",
    "- A neural network is composed of many simple neurons, so that the output of a neuron can be the input of another\n",
    "- The sample neural network model has 3 input nodes, 3 hidden units, and 1 output unit\n",
    "    - input layer: the leftmost layer\n",
    "    - outout layer: the rightmost layer (produce target, i.e. prediction, classification)\n",
    "    - bias units: indicated by \"+1\" node\n",
    "    - hidden layer: the middle layer of nodes\n",
    "<img src=\"neural_network.png\" width=\"60%\"/>\n",
    "\n",
    "- $W$, $x$, and $b$ usually represented as arrays (i.e. vectorized)\n",
    "   - $w_{ij}^{(l)}$: the weight associated with the link from unit $j$ in layer $l$ to unit $i$ in layer $l+1$\n",
    "   - $W^{(1)} \\in \\mathbb{R}^{3\\text{x}3}$, $W^{(2)} \\in \\mathbb{R}^{1\\text{x}3}$, $b^{(1)} \\in \\mathbb{R}^{3\\text{x}1}$, $b^{(2)} \\in \\mathbb{R}^{1\\text{x}1}$\n",
    "   - Note $W^{(l)}x$ is the dot product between $W^{(l)}$ and $x$, i.e. $W^{(l)} \\cdot x$\n",
    "   \n",
    "- If a neural network contains more than 1 hidden layer, it's called a **deep neural network** (**deep learning**)\n",
    "- Training a neural network model is to find $W$ and $b$ that optimize some **cost function**, given tranining samples (X,Y), where X and Y can be multi-dimensional\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Cost function\n",
    "- Training set: m samples denoted as $(X,Y)={(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})}$\n",
    "- A typical cost function: **mean_squared_error** \n",
    "  - Sum of square error: $J(W,b;x,y)=\\frac{1}{2}||h_{W,b}(x)-y||^2$\n",
    "  - Regularization (square of each weight, or L2): $\\sum_{i, j, l}(w_{ij}^{(l)})^2$. An important mechanism to prevent overfitting\n",
    "  - Cost function:\n",
    "$$J(W,b)=\\frac{1}{m}\\sum_i^m{(\\frac{1}{2}||h_{W,b}(x)-y||^2)}+ \\frac{\\lambda}{2}\\sum_{i, j, l}(w_{ij}^{(l)})^2$$, where $\\lambda$ is **regularization coefficient**\n",
    "- Other popular cost functions\n",
    "  - **Cross-entropy cost**\n",
    "      - Let's assume a single neuron with sigmoid activation function <img src='single_neuron.png' width=\"30%\" style=\"float: right;\">\n",
    "      - Let $\\widehat y=h_{W,b}(x)$, the prediction of true value $y$. $\\widehat y, y \\in [0,1]$. \n",
    "      - Then cross-entrophy cost is defined as: $$J=-\\frac{1}{m}\\sum_{i=1}^m{y_i\\ln{\\widehat y_i}+(1-y_i)\\ln{(1-\\widehat y_i)}}$$\n",
    "      - What makes cross-entropy a good cost function\n",
    "        - It's non-negative\n",
    "        - if the neuron's output $\\widehat y$ is close to the actual value $y$ (0 or 1) for all training inputs, then the cross-entropy will be close to zero\n",
    "- For comparison between \"Sum of Square error\" and \"Cross-entropy cost\", read http://neuralnetworksanddeeplearning.com/chap3.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Gradient Descent\n",
    "- An optimization algorithm used to find the values of parameters ($W, b$) of a function ($J$) that minimizes a cost function ($J(W,b)$.\n",
    "- It is best used when the parameters cannot be calculated analytically (e.g. using linear algebra) and must be searched for by an optimization algorithm\n",
    "  <img src='gradient_descent.png' width='80%'>\n",
    "  resource: https://www.analyticsvidhya.com/blog/2017/03/introduction-to-gradient-descent-algorithm-along-its-variants/\n",
    "- It uses derivatives of cost function to determine the direction to move the parameter values in order to get a lower cost on the next iteration\n",
    "- Procedure:\n",
    "    1. initialize $W$ with random values\n",
    "    2. given samples (X,Y) as inputs, calculate dirivatives of cost function with regard to every parameter $w_{ij}^{(l)}$, i.e. $\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$\n",
    "    3. update parameters by $(w_{ij}^{(l)})^{'}=w_{ij}^{(l)}-\\alpha*\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$, where $\\alpha$ is the learning rate\n",
    "    4. repeat steps 2-3 until $w_{ij}^{(l)}$ converges\n",
    "- **Learning rate $\\alpha$**\n",
    "  - It's critical to pick the right learning rate. Big $\\alpha$ or small $\\alpha$?\n",
    "  - $\\alpha$ may need to be adapted as learning unfolds\n",
    "- Challenges of Gradient Descent\n",
    "  - It is expensive to compute $\\frac{1}{m}\\sum_i^m{(\\frac{1}{2}||h_{W,b}(x_i)-y_i||^2)}$ for all samples in each round\n",
    "  - It is difficult to compute $\\frac{\\partial{J}}{\\partial{w_{ij}^{(l)}}}$ if a neural netowrk has many layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Stochastic Gradient Descent\n",
    "- Estimate of cost function using a subset of randomly chosen training samples (mini-batch) instead of the entire training set\n",
    "- Procedure: \n",
    "  1. pick a randomly selected mini-batch, train with them and update $W, b$, \n",
    "  2. repeat step (1) with another randomly selected mini-batch until the training set is exhausted (i.e. complete an epoch), \n",
    "  3. start over with another epoch until $W, b$ converge\n",
    "- **Hyperparameters** (parameters that control the learning of $W, b$)\n",
    "    - **Batch size**: the size of samples selected for each iteration\n",
    "    - **Epoches**: One epoch means one complete pass through the whole training set. Ususally we need to use many epoches until $W, b$ converge\n",
    "    - e.g. if your sample size is 1000, and your batch size is 200, how many iterations are needed for one epoch?\n",
    "    - e.g. if you set # of epoches to 5, how many times in total you update $W, b$?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6. Backpropagation Algorithm -- The efficient way to calcluate gradients (i.e. partial derivatives) \n",
    "An intutive video: https://www.youtube.com/watch?v=tIeHLnjs5U8\n",
    "\n",
    "Forward Propagation             |  Backprogation\n",
    ":-------------------------:|:-------------------------:\n",
    "![](forward-propagation.png)  |  ![](backpropagation.png)\n",
    "input signals are passing through each layer by multiplying the weights | backpropagate the error back to each layer proportional to perspective weights, and update the weights based on attributed errors in hope to correct the error\n",
    "- Algorithm:\n",
    "  1. perform a feedforward pass, computing the activations for layers L2, L3, ... and so on up to the output layer\n",
    "  2. for output layer $n$,<br> $\\delta^{(n)} = \\frac{\\partial}{\\partial z^{(n)}}\n",
    " J(W,b; x, y) = \\frac{\\partial}{\\partial z^{(n)}}\n",
    " \\frac{1}{2} \\left\\|y - h_{W,b}(x)\\right\\|^2 = - (y - a^{(n)}) \\cdot f'(z^{(n)})$\n",
    "  3. for $l=n-1, n-2, ..., n-3, ..., 2$,<br>\n",
    "  $ \\delta^{(l)} = \\left((W^{(l)})^T \\delta^{(l+1)}\\right) \\cdot f'(z^{(l)})$\n",
    "  4. Compute the desired partial derivatives, which are given as:<br>\n",
    "     $ \\frac{\\partial}{\\partial W_{ij}^{(l)}} J(W,b; x, y) = a^{(l)}_j \\delta_i^{(l+1)}$ <br>\n",
    "$\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; x, y) = \\delta_i^{(l+1)}$\n",
    "- Example: \n",
    "  - $\\delta^{(3)} = \\frac{\\partial}{\\partial z^{(3)}} J(W,b; x, y) = (a^{(3)} - y) \\cdot f'(z^{(3)})$\n",
    "\n",
    "  - $ \\delta^{(2)} = \\left((W^{(2)})^T \\delta^{(3)}\\right) \\cdot f'(z^{(2)})$\n",
    "  - $ \\frac{\\partial}{\\partial W_{12}^{(2)}} J(W,b; x, y) = a^{(2)}_2 \\delta_1^{(3)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Hyperparameters\n",
    "- Hyperparameters are parameters that control the learning of $w, b$ (our learning target)\n",
    "- Summary of hyperparameters:\n",
    "    - Network structure:\n",
    "      - number of hidden layers\n",
    "      - number of neurons of each layer\n",
    "      - activation fucntion of each layer\n",
    "    - Learning rate ($\\alpha$)\n",
    "    - regularization coeffiecient ($\\lambda$)\n",
    "    - mini-batch size\n",
    "    - epoches\n",
    "- For detailed explanation, watch: https://www.coursera.org/learn/neural-networks-deep-learning/lecture/TBvb5/parameters-vs-hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Develop your First Neural Network Model with Keras\n",
    "- Keras: \n",
    "  - high-level library for neural network models\n",
    "  - It wraps the efficient numerical computation libraries Theano and TensorFlow \n",
    "- Why Keras:\n",
    "  - Simple to get started and keep going\n",
    "  - Written in python and higly modular; easy to expand\n",
    "  - Built-in modules for some sophisticated neural network models\n",
    "- Installation\n",
    "  - pip install keras (or pip install keras --upgrade if you already have it) to install the latest version \n",
    "  - pip install theano \n",
    "  - pip install tensorflow \n",
    "  - pip install np-utils \n",
    "- Basic procedure\n",
    "  1. Load data\n",
    "  2. Define model\n",
    "  3. Compile model\n",
    "  4. Fit model\n",
    "  5. Evaluate model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Basic Keras Modeling Constructs\n",
    "- Sequential model:  linear stack of layers\n",
    "- Layers\n",
    "  - Dense: in a dense layer, each neuron is connected to neurons in the next layer\n",
    "  - Embedding\n",
    "  - Convolution\n",
    "  - MaxPooling\n",
    "  - ...\n",
    "- Cost (loss) functions\n",
    "  - mean_squared_error\n",
    "  - binary_crossentropy\n",
    "  - categorical_crossentropy\n",
    "  - ...\n",
    "- Optimizer (i.e. optimization algorithm)\n",
    "  - SGD (Stochastic Gradient Descent): fixed learning rate in all iterations\n",
    "  - Adagrad: adapts the learning rate to the parameters, performing larger updates for infrequent, and smaller updates for frequent parameters\n",
    "  - Adam (Adaptive Moment Estimation): computes adaptive learning rates for each parameter.\n",
    "- Metrics\n",
    "  - accuracy: a ratio of correctly predicted samples to the total samples\n",
    "  - precision/recall/f1 through sklearn package\n",
    "  - Example:\n",
    "    - acc: (90+85)/200=87%\n",
    "    - prec: \n",
    "    - recall:\n",
    "\n",
    "|        | Predicted T        |   Predicted F  |\n",
    "|:----------|-------------------:|---------------:|\n",
    "|Actual T  |  90                | 10              |\n",
    "|Actual F  |  15                | 85              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Example\n",
    "- Example: build a simple neural network model to predict diabetes using \"Pima Indians onset of diabetes database\" at http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes\n",
    "  - Columns 1-8: variables \n",
    "  - Column 9: class variable, 0 or 1\n",
    "- A sequential model with 4 layers\n",
    "  - each node is a tensor, a function of multidimensional arrays\n",
    "    - Input (L1)\n",
    "    - L2 (hidden layer, dense)\n",
    "    - L3 (hidden layer, dense)\n",
    "    - Output (dense)\n",
    "  - the model is a tensor graph (computation graph)\n",
    "\n",
    "  <img src='model.png' width='20%'>\n",
    "  <div class=\"alert alert-block alert-info\">Training a deep learning model is a very empirical process. You may need to tune the hyperparameters in many iterations</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up interactive shell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1    268\n",
       "Name: 8, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(768, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 3.1. Load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load data\n",
    "data=pd.read_csv(\"../../../dataset/pima-indians-diabetes.csv\",header=None)\n",
    "data.head()\n",
    "\n",
    "data[8].value_counts()\n",
    "\n",
    "X=data.values[:,0:8]\n",
    "y=data.values[:,8]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rliu/anaconda/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/rliu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1188: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/rliu/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.2. Create Model\n",
    "\n",
    "# sequential model is a linear stack of layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "# in a dense layer which each neuron is connected to \n",
    "# each neuron in the next layer\n",
    "from keras.layers import Dense\n",
    "\n",
    "# import packages for L2 regularization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "# set lambda (regularization coefficient)\n",
    "lam=0.01\n",
    "\n",
    "# create a sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add a dense layer with 12 neurons, 8 input variables\n",
    "# and rectifier activation function (relu)\n",
    "# and L2 regularization\n",
    "# how many parameters in this layer?\n",
    "model.add(Dense(12, input_dim=8, \\\n",
    "                activation='relu', \\\n",
    "                kernel_regularizer=l2(lam), \\\n",
    "                name='L2') )\n",
    "\n",
    "# add another hidden layer with 8 neurons\n",
    "model.add(Dense(8, activation='relu', \\\n",
    "                kernel_regularizer=l2(lam),\\\n",
    "                name='L3') )\n",
    "\n",
    "# add the output layer with sigmoid activation function\n",
    "# to return probability\n",
    "model.add(Dense(1, activation='sigmoid', \\\n",
    "                name='Output'))\n",
    "\n",
    "# compile the model using binary corss entropy cost function\n",
    "# adam optimizer and accuracy\n",
    "model.compile(loss='binary_crossentropy', \\\n",
    "              optimizer='adam', \\\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "L2 (Dense)                   (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "L3 (Dense)                   (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "Output (Dense)               (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.3. Check model configuration\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Show the model in a computation graph\n",
    "# it needs pydot and graphviz\n",
    "# don't worry if you don't have them installed\n",
    "\n",
    "#from keras.utils import plot_model\n",
    "#plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 576 samples, validate on 192 samples\n",
      "Epoch 1/150\n",
      "0s - loss: 5.0593 - acc: 0.6615 - val_loss: 4.8492 - val_acc: 0.6146\n",
      "Epoch 2/150\n",
      "0s - loss: 3.3136 - acc: 0.5955 - val_loss: 3.3641 - val_acc: 0.5208\n",
      "Epoch 3/150\n",
      "0s - loss: 1.8646 - acc: 0.5434 - val_loss: 1.5383 - val_acc: 0.5000\n",
      "Epoch 4/150\n",
      "0s - loss: 1.2648 - acc: 0.5799 - val_loss: 1.2490 - val_acc: 0.5312\n",
      "Epoch 5/150\n",
      "0s - loss: 1.0285 - acc: 0.6042 - val_loss: 1.0567 - val_acc: 0.5417\n",
      "Epoch 6/150\n",
      "0s - loss: 0.9132 - acc: 0.6181 - val_loss: 0.9434 - val_acc: 0.5990\n",
      "Epoch 7/150\n",
      "0s - loss: 0.8696 - acc: 0.6441 - val_loss: 0.9042 - val_acc: 0.5781\n",
      "Epoch 8/150\n",
      "0s - loss: 0.8435 - acc: 0.6476 - val_loss: 0.8860 - val_acc: 0.6094\n",
      "Epoch 9/150\n",
      "0s - loss: 0.8111 - acc: 0.6632 - val_loss: 0.8955 - val_acc: 0.6458\n",
      "Epoch 10/150\n",
      "0s - loss: 0.8007 - acc: 0.6892 - val_loss: 0.8708 - val_acc: 0.6406\n",
      "Epoch 11/150\n",
      "0s - loss: 0.7828 - acc: 0.6823 - val_loss: 0.8381 - val_acc: 0.6146\n",
      "Epoch 12/150\n",
      "0s - loss: 0.7680 - acc: 0.6892 - val_loss: 0.8299 - val_acc: 0.6667\n",
      "Epoch 13/150\n",
      "0s - loss: 0.7591 - acc: 0.6649 - val_loss: 0.8488 - val_acc: 0.6667\n",
      "Epoch 14/150\n",
      "0s - loss: 0.7544 - acc: 0.6684 - val_loss: 0.8606 - val_acc: 0.6458\n",
      "Epoch 15/150\n",
      "0s - loss: 0.7568 - acc: 0.6493 - val_loss: 0.8552 - val_acc: 0.6615\n",
      "Epoch 16/150\n",
      "0s - loss: 0.7284 - acc: 0.6962 - val_loss: 0.8074 - val_acc: 0.6615\n",
      "Epoch 17/150\n",
      "0s - loss: 0.7116 - acc: 0.6944 - val_loss: 0.7963 - val_acc: 0.6510\n",
      "Epoch 18/150\n",
      "0s - loss: 0.7320 - acc: 0.6736 - val_loss: 0.7672 - val_acc: 0.6979\n",
      "Epoch 19/150\n",
      "0s - loss: 0.7166 - acc: 0.6927 - val_loss: 0.7678 - val_acc: 0.6771\n",
      "Epoch 20/150\n",
      "0s - loss: 0.6963 - acc: 0.7014 - val_loss: 0.8036 - val_acc: 0.6510\n",
      "Epoch 21/150\n",
      "0s - loss: 0.6984 - acc: 0.6997 - val_loss: 0.7668 - val_acc: 0.6615\n",
      "Epoch 22/150\n",
      "0s - loss: 0.6900 - acc: 0.6892 - val_loss: 0.7656 - val_acc: 0.6927\n",
      "Epoch 23/150\n",
      "0s - loss: 0.6944 - acc: 0.6944 - val_loss: 0.7571 - val_acc: 0.6927\n",
      "Epoch 24/150\n",
      "0s - loss: 0.6788 - acc: 0.7240 - val_loss: 0.7424 - val_acc: 0.6875\n",
      "Epoch 25/150\n",
      "0s - loss: 0.6835 - acc: 0.7083 - val_loss: 0.7465 - val_acc: 0.6771\n",
      "Epoch 26/150\n",
      "0s - loss: 0.6804 - acc: 0.7031 - val_loss: 0.7307 - val_acc: 0.6875\n",
      "Epoch 27/150\n",
      "0s - loss: 0.6696 - acc: 0.7153 - val_loss: 0.7549 - val_acc: 0.6719\n",
      "Epoch 28/150\n",
      "0s - loss: 0.6664 - acc: 0.7153 - val_loss: 0.7315 - val_acc: 0.6927\n",
      "Epoch 29/150\n",
      "0s - loss: 0.6671 - acc: 0.7205 - val_loss: 0.7204 - val_acc: 0.6927\n",
      "Epoch 30/150\n",
      "0s - loss: 0.6704 - acc: 0.7240 - val_loss: 0.7361 - val_acc: 0.6823\n",
      "Epoch 31/150\n",
      "0s - loss: 0.6568 - acc: 0.6997 - val_loss: 0.7233 - val_acc: 0.6510\n",
      "Epoch 32/150\n",
      "0s - loss: 0.6670 - acc: 0.7066 - val_loss: 0.7498 - val_acc: 0.6562\n",
      "Epoch 33/150\n",
      "0s - loss: 0.6735 - acc: 0.7049 - val_loss: 0.7177 - val_acc: 0.6667\n",
      "Epoch 34/150\n",
      "0s - loss: 0.6619 - acc: 0.7170 - val_loss: 0.7313 - val_acc: 0.6771\n",
      "Epoch 35/150\n",
      "0s - loss: 0.6530 - acc: 0.7240 - val_loss: 0.7219 - val_acc: 0.6771\n",
      "Epoch 36/150\n",
      "0s - loss: 0.6521 - acc: 0.7188 - val_loss: 0.7241 - val_acc: 0.6510\n",
      "Epoch 37/150\n",
      "0s - loss: 0.6441 - acc: 0.7205 - val_loss: 0.7191 - val_acc: 0.6875\n",
      "Epoch 38/150\n",
      "0s - loss: 0.6477 - acc: 0.7135 - val_loss: 0.7236 - val_acc: 0.6667\n",
      "Epoch 39/150\n",
      "0s - loss: 0.6564 - acc: 0.7205 - val_loss: 0.7389 - val_acc: 0.6562\n",
      "Epoch 40/150\n",
      "0s - loss: 0.6481 - acc: 0.7257 - val_loss: 0.6948 - val_acc: 0.6927\n",
      "Epoch 41/150\n",
      "0s - loss: 0.6360 - acc: 0.7309 - val_loss: 0.7153 - val_acc: 0.6719\n",
      "Epoch 42/150\n",
      "0s - loss: 0.6455 - acc: 0.7170 - val_loss: 0.7369 - val_acc: 0.6771\n",
      "Epoch 43/150\n",
      "0s - loss: 0.6384 - acc: 0.7257 - val_loss: 0.6997 - val_acc: 0.6719\n",
      "Epoch 44/150\n",
      "0s - loss: 0.6322 - acc: 0.7083 - val_loss: 0.6917 - val_acc: 0.6979\n",
      "Epoch 45/150\n",
      "0s - loss: 0.6355 - acc: 0.7292 - val_loss: 0.6912 - val_acc: 0.6771\n",
      "Epoch 46/150\n",
      "0s - loss: 0.6354 - acc: 0.7326 - val_loss: 0.6916 - val_acc: 0.7083\n",
      "Epoch 47/150\n",
      "0s - loss: 0.6310 - acc: 0.7309 - val_loss: 0.6927 - val_acc: 0.6771\n",
      "Epoch 48/150\n",
      "0s - loss: 0.6320 - acc: 0.7274 - val_loss: 0.6830 - val_acc: 0.6979\n",
      "Epoch 49/150\n",
      "0s - loss: 0.6312 - acc: 0.7274 - val_loss: 0.6861 - val_acc: 0.7031\n",
      "Epoch 50/150\n",
      "0s - loss: 0.6297 - acc: 0.7292 - val_loss: 0.6802 - val_acc: 0.6927\n",
      "Epoch 51/150\n",
      "0s - loss: 0.6278 - acc: 0.7309 - val_loss: 0.6845 - val_acc: 0.6823\n",
      "Epoch 52/150\n",
      "0s - loss: 0.6388 - acc: 0.7170 - val_loss: 0.6919 - val_acc: 0.6771\n",
      "Epoch 53/150\n",
      "0s - loss: 0.6280 - acc: 0.7274 - val_loss: 0.6768 - val_acc: 0.7188\n",
      "Epoch 54/150\n",
      "0s - loss: 0.6256 - acc: 0.7309 - val_loss: 0.6970 - val_acc: 0.6927\n",
      "Epoch 55/150\n",
      "0s - loss: 0.6247 - acc: 0.7344 - val_loss: 0.6751 - val_acc: 0.6979\n",
      "Epoch 56/150\n",
      "0s - loss: 0.6322 - acc: 0.7274 - val_loss: 0.6698 - val_acc: 0.7188\n",
      "Epoch 57/150\n",
      "0s - loss: 0.6285 - acc: 0.7205 - val_loss: 0.6795 - val_acc: 0.7031\n",
      "Epoch 58/150\n",
      "0s - loss: 0.6189 - acc: 0.7257 - val_loss: 0.6728 - val_acc: 0.7292\n",
      "Epoch 59/150\n",
      "0s - loss: 0.6324 - acc: 0.7240 - val_loss: 0.6797 - val_acc: 0.6979\n",
      "Epoch 60/150\n",
      "0s - loss: 0.6202 - acc: 0.7222 - val_loss: 0.6867 - val_acc: 0.6615\n",
      "Epoch 61/150\n",
      "0s - loss: 0.6246 - acc: 0.7240 - val_loss: 0.6813 - val_acc: 0.6875\n",
      "Epoch 62/150\n",
      "0s - loss: 0.6407 - acc: 0.7170 - val_loss: 0.6909 - val_acc: 0.6979\n",
      "Epoch 63/150\n",
      "0s - loss: 0.6157 - acc: 0.7344 - val_loss: 0.6670 - val_acc: 0.7031\n",
      "Epoch 64/150\n",
      "0s - loss: 0.6126 - acc: 0.7431 - val_loss: 0.6904 - val_acc: 0.6979\n",
      "Epoch 65/150\n",
      "0s - loss: 0.6313 - acc: 0.7153 - val_loss: 0.6980 - val_acc: 0.6979\n",
      "Epoch 66/150\n",
      "0s - loss: 0.6324 - acc: 0.7309 - val_loss: 0.6988 - val_acc: 0.6823\n",
      "Epoch 67/150\n",
      "0s - loss: 0.6252 - acc: 0.7240 - val_loss: 0.7038 - val_acc: 0.6875\n",
      "Epoch 68/150\n",
      "0s - loss: 0.6343 - acc: 0.7031 - val_loss: 0.7461 - val_acc: 0.6719\n",
      "Epoch 69/150\n",
      "0s - loss: 0.6297 - acc: 0.7222 - val_loss: 0.6771 - val_acc: 0.6875\n",
      "Epoch 70/150\n",
      "0s - loss: 0.6124 - acc: 0.7292 - val_loss: 0.6736 - val_acc: 0.6979\n",
      "Epoch 71/150\n",
      "0s - loss: 0.6108 - acc: 0.7326 - val_loss: 0.6626 - val_acc: 0.7188\n",
      "Epoch 72/150\n",
      "0s - loss: 0.6161 - acc: 0.7292 - val_loss: 0.6931 - val_acc: 0.6771\n",
      "Epoch 73/150\n",
      "0s - loss: 0.6172 - acc: 0.7292 - val_loss: 0.6713 - val_acc: 0.6875\n",
      "Epoch 74/150\n",
      "0s - loss: 0.6330 - acc: 0.7222 - val_loss: 0.6812 - val_acc: 0.6823\n",
      "Epoch 75/150\n",
      "0s - loss: 0.6161 - acc: 0.7101 - val_loss: 0.6711 - val_acc: 0.6979\n",
      "Epoch 76/150\n",
      "0s - loss: 0.6118 - acc: 0.7361 - val_loss: 0.6559 - val_acc: 0.7240\n",
      "Epoch 77/150\n",
      "0s - loss: 0.6214 - acc: 0.7309 - val_loss: 0.6808 - val_acc: 0.6927\n",
      "Epoch 78/150\n",
      "0s - loss: 0.6147 - acc: 0.7292 - val_loss: 0.6919 - val_acc: 0.6927\n",
      "Epoch 79/150\n",
      "0s - loss: 0.6113 - acc: 0.7378 - val_loss: 0.6780 - val_acc: 0.7083\n",
      "Epoch 80/150\n",
      "0s - loss: 0.6074 - acc: 0.7309 - val_loss: 0.6666 - val_acc: 0.7031\n",
      "Epoch 81/150\n",
      "0s - loss: 0.6065 - acc: 0.7188 - val_loss: 0.6658 - val_acc: 0.6875\n",
      "Epoch 82/150\n",
      "0s - loss: 0.6040 - acc: 0.7240 - val_loss: 0.6729 - val_acc: 0.7292\n",
      "Epoch 83/150\n",
      "0s - loss: 0.6068 - acc: 0.7361 - val_loss: 0.6480 - val_acc: 0.7188\n",
      "Epoch 84/150\n",
      "0s - loss: 0.6026 - acc: 0.7170 - val_loss: 0.6618 - val_acc: 0.6927\n",
      "Epoch 85/150\n",
      "0s - loss: 0.6018 - acc: 0.7500 - val_loss: 0.6971 - val_acc: 0.6927\n",
      "Epoch 86/150\n",
      "0s - loss: 0.6025 - acc: 0.7361 - val_loss: 0.6715 - val_acc: 0.6927\n",
      "Epoch 87/150\n",
      "0s - loss: 0.6167 - acc: 0.7240 - val_loss: 0.6541 - val_acc: 0.7240\n",
      "Epoch 88/150\n",
      "0s - loss: 0.5975 - acc: 0.7344 - val_loss: 0.6523 - val_acc: 0.7135\n",
      "Epoch 89/150\n",
      "0s - loss: 0.5937 - acc: 0.7378 - val_loss: 0.6522 - val_acc: 0.7031\n",
      "Epoch 90/150\n",
      "0s - loss: 0.5991 - acc: 0.7309 - val_loss: 0.6552 - val_acc: 0.7031\n",
      "Epoch 91/150\n",
      "0s - loss: 0.6020 - acc: 0.7413 - val_loss: 0.6607 - val_acc: 0.7188\n",
      "Epoch 92/150\n",
      "0s - loss: 0.5946 - acc: 0.7500 - val_loss: 0.6442 - val_acc: 0.7083\n",
      "Epoch 93/150\n",
      "0s - loss: 0.5994 - acc: 0.7396 - val_loss: 0.6494 - val_acc: 0.7292\n",
      "Epoch 94/150\n",
      "0s - loss: 0.5981 - acc: 0.7292 - val_loss: 0.6527 - val_acc: 0.7083\n",
      "Epoch 95/150\n",
      "0s - loss: 0.5905 - acc: 0.7292 - val_loss: 0.6458 - val_acc: 0.7135\n",
      "Epoch 96/150\n",
      "0s - loss: 0.6068 - acc: 0.7153 - val_loss: 0.6397 - val_acc: 0.6979\n",
      "Epoch 97/150\n",
      "0s - loss: 0.6007 - acc: 0.7431 - val_loss: 0.6575 - val_acc: 0.7135\n",
      "Epoch 98/150\n",
      "0s - loss: 0.5993 - acc: 0.7309 - val_loss: 0.6612 - val_acc: 0.6979\n",
      "Epoch 99/150\n",
      "0s - loss: 0.5968 - acc: 0.7361 - val_loss: 0.6945 - val_acc: 0.6875\n",
      "Epoch 100/150\n",
      "0s - loss: 0.6143 - acc: 0.7153 - val_loss: 0.7618 - val_acc: 0.6719\n",
      "Epoch 101/150\n",
      "0s - loss: 0.6133 - acc: 0.7188 - val_loss: 0.7340 - val_acc: 0.6615\n",
      "Epoch 102/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0s - loss: 0.6163 - acc: 0.7240 - val_loss: 0.6751 - val_acc: 0.6927\n",
      "Epoch 103/150\n",
      "0s - loss: 0.6000 - acc: 0.7292 - val_loss: 0.6524 - val_acc: 0.7240\n",
      "Epoch 104/150\n",
      "0s - loss: 0.5820 - acc: 0.7483 - val_loss: 0.6641 - val_acc: 0.6979\n",
      "Epoch 105/150\n",
      "0s - loss: 0.5875 - acc: 0.7431 - val_loss: 0.6409 - val_acc: 0.7188\n",
      "Epoch 106/150\n",
      "0s - loss: 0.5877 - acc: 0.7483 - val_loss: 0.6656 - val_acc: 0.6979\n",
      "Epoch 107/150\n",
      "0s - loss: 0.5886 - acc: 0.7396 - val_loss: 0.6414 - val_acc: 0.7240\n",
      "Epoch 108/150\n",
      "0s - loss: 0.5852 - acc: 0.7517 - val_loss: 0.6776 - val_acc: 0.6927\n",
      "Epoch 109/150\n",
      "0s - loss: 0.5874 - acc: 0.7326 - val_loss: 0.6422 - val_acc: 0.7292\n",
      "Epoch 110/150\n",
      "0s - loss: 0.5895 - acc: 0.7326 - val_loss: 0.6443 - val_acc: 0.7344\n",
      "Epoch 111/150\n",
      "0s - loss: 0.5970 - acc: 0.7274 - val_loss: 0.6330 - val_acc: 0.7344\n",
      "Epoch 112/150\n",
      "0s - loss: 0.5849 - acc: 0.7448 - val_loss: 0.6344 - val_acc: 0.7396\n",
      "Epoch 113/150\n",
      "0s - loss: 0.5887 - acc: 0.7378 - val_loss: 0.6380 - val_acc: 0.7292\n",
      "Epoch 114/150\n",
      "0s - loss: 0.5780 - acc: 0.7500 - val_loss: 0.6352 - val_acc: 0.7344\n",
      "Epoch 115/150\n",
      "0s - loss: 0.5822 - acc: 0.7431 - val_loss: 0.6349 - val_acc: 0.7448\n",
      "Epoch 116/150\n",
      "0s - loss: 0.5830 - acc: 0.7535 - val_loss: 0.6630 - val_acc: 0.7240\n",
      "Epoch 117/150\n",
      "0s - loss: 0.5825 - acc: 0.7465 - val_loss: 0.6478 - val_acc: 0.7083\n",
      "Epoch 118/150\n",
      "0s - loss: 0.5763 - acc: 0.7396 - val_loss: 0.6330 - val_acc: 0.7396\n",
      "Epoch 119/150\n",
      "0s - loss: 0.5828 - acc: 0.7344 - val_loss: 0.6487 - val_acc: 0.7240\n",
      "Epoch 120/150\n",
      "0s - loss: 0.5796 - acc: 0.7448 - val_loss: 0.6265 - val_acc: 0.7240\n",
      "Epoch 121/150\n",
      "0s - loss: 0.5779 - acc: 0.7292 - val_loss: 0.6412 - val_acc: 0.7292\n",
      "Epoch 122/150\n",
      "0s - loss: 0.5806 - acc: 0.7361 - val_loss: 0.6348 - val_acc: 0.7188\n",
      "Epoch 123/150\n",
      "0s - loss: 0.5721 - acc: 0.7465 - val_loss: 0.6369 - val_acc: 0.7448\n",
      "Epoch 124/150\n",
      "0s - loss: 0.5964 - acc: 0.7309 - val_loss: 0.6450 - val_acc: 0.7396\n",
      "Epoch 125/150\n",
      "0s - loss: 0.6017 - acc: 0.7309 - val_loss: 0.6416 - val_acc: 0.7135\n",
      "Epoch 126/150\n",
      "0s - loss: 0.5778 - acc: 0.7465 - val_loss: 0.6459 - val_acc: 0.7188\n",
      "Epoch 127/150\n",
      "0s - loss: 0.5842 - acc: 0.7326 - val_loss: 0.6368 - val_acc: 0.7188\n",
      "Epoch 128/150\n",
      "0s - loss: 0.5762 - acc: 0.7396 - val_loss: 0.6342 - val_acc: 0.7240\n",
      "Epoch 129/150\n",
      "0s - loss: 0.5702 - acc: 0.7483 - val_loss: 0.6270 - val_acc: 0.7240\n",
      "Epoch 130/150\n",
      "0s - loss: 0.5782 - acc: 0.7535 - val_loss: 0.6417 - val_acc: 0.7344\n",
      "Epoch 131/150\n",
      "0s - loss: 0.5777 - acc: 0.7465 - val_loss: 0.6312 - val_acc: 0.7344\n",
      "Epoch 132/150\n",
      "0s - loss: 0.5683 - acc: 0.7431 - val_loss: 0.6350 - val_acc: 0.7135\n",
      "Epoch 133/150\n",
      "0s - loss: 0.5839 - acc: 0.7465 - val_loss: 0.6499 - val_acc: 0.7500\n",
      "Epoch 134/150\n",
      "0s - loss: 0.5690 - acc: 0.7587 - val_loss: 0.6203 - val_acc: 0.7240\n",
      "Epoch 135/150\n",
      "0s - loss: 0.5646 - acc: 0.7483 - val_loss: 0.6249 - val_acc: 0.7292\n",
      "Epoch 136/150\n",
      "0s - loss: 0.5668 - acc: 0.7517 - val_loss: 0.6174 - val_acc: 0.7240\n",
      "Epoch 137/150\n",
      "0s - loss: 0.5839 - acc: 0.7378 - val_loss: 0.6230 - val_acc: 0.7396\n",
      "Epoch 138/150\n",
      "0s - loss: 0.5701 - acc: 0.7500 - val_loss: 0.6202 - val_acc: 0.7188\n",
      "Epoch 139/150\n",
      "0s - loss: 0.5625 - acc: 0.7639 - val_loss: 0.6377 - val_acc: 0.7135\n",
      "Epoch 140/150\n",
      "0s - loss: 0.5769 - acc: 0.7465 - val_loss: 0.6280 - val_acc: 0.7188\n",
      "Epoch 141/150\n",
      "0s - loss: 0.5673 - acc: 0.7483 - val_loss: 0.6165 - val_acc: 0.7344\n",
      "Epoch 142/150\n",
      "0s - loss: 0.5766 - acc: 0.7378 - val_loss: 0.6260 - val_acc: 0.7500\n",
      "Epoch 143/150\n",
      "0s - loss: 0.5837 - acc: 0.7396 - val_loss: 0.6243 - val_acc: 0.7344\n",
      "Epoch 144/150\n",
      "0s - loss: 0.5758 - acc: 0.7465 - val_loss: 0.6236 - val_acc: 0.7344\n",
      "Epoch 145/150\n",
      "0s - loss: 0.5659 - acc: 0.7604 - val_loss: 0.6435 - val_acc: 0.7292\n",
      "Epoch 146/150\n",
      "0s - loss: 0.5703 - acc: 0.7413 - val_loss: 0.6663 - val_acc: 0.7188\n",
      "Epoch 147/150\n",
      "0s - loss: 0.5810 - acc: 0.7448 - val_loss: 0.6782 - val_acc: 0.7135\n",
      "Epoch 148/150\n",
      "0s - loss: 0.5669 - acc: 0.7500 - val_loss: 0.6448 - val_acc: 0.7240\n",
      "Epoch 149/150\n",
      "0s - loss: 0.5653 - acc: 0.7500 - val_loss: 0.6155 - val_acc: 0.7448\n",
      "Epoch 150/150\n",
      "0s - loss: 0.5763 - acc: 0.7396 - val_loss: 0.6117 - val_acc: 0.7292\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.4. Fit Model\n",
    "\n",
    "# train the model with min-batch of size 10, \n",
    "# 100 epoches (# how many iterations?)\n",
    "# Keep 20% samples for test\n",
    "# shuffle data before train-test split\n",
    "# set fitting history into variable \"training\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, \\\n",
    "                                test_size=0.25, random_state=123)\n",
    "\n",
    "training=model.fit(X_train, y_train, \\\n",
    "                   validation_data=[X_test, y_test], \\\n",
    "                   shuffle=True,epochs=150, \\\n",
    "                   batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/192 [====>.........................] - ETA: 0s\n",
      "acc: 72.92%\n",
      "[[0.7468275 ]\n",
      " [0.44640124]\n",
      " [0.72950506]\n",
      " [0.265513  ]\n",
      " [0.11131728]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.80      0.79       119\n",
      "          1       0.65      0.62      0.63        73\n",
      "\n",
      "avg / total       0.73      0.73      0.73       192\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3.5. Get prediction and performance\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "# evaluate the model using samples\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], \\\n",
    "                        scores[1]*100))\n",
    "\n",
    "# get prediction\n",
    "predicted=model.predict(X_test)\n",
    "print(predicted[0:5])\n",
    "# reshape the 2-dimension array to 1-dimension\n",
    "predicted=np.reshape(predicted, -1)\n",
    "\n",
    "# decide prediction to be 1 or 0 based probability\n",
    "predicted=np.where(predicted>0.5, 1, 0)\n",
    "\n",
    "# calculate performance report\n",
    "print(metrics.classification_report(y_test, predicted, \\\n",
    "                                    labels=[0,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
