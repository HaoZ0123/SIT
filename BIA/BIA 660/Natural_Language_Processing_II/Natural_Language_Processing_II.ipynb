{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Natural Language Processing Using NLTK (II)</center>\n",
    "\n",
    "References:\n",
    " - http://www.nltk.org/book_1ed/\n",
    " - https://web.stanford.edu/class/cs124/lec/Information_Extraction_and_Named_Entity_Recognition.pdf\n",
    " - https://nlpforhackers.io/complete-guide-to-spacy/\n",
    " - https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Objectives and Basic Steps\n",
    "\n",
    " - Objectives:\n",
    "   * Split documents into words, punctuation sysmbols, or segments\n",
    "   * Understand vocabulary of the text\n",
    "   * Extract features for further text mining tasks\n",
    " - Basic processing steps:\n",
    "   * Tokenization: split documents into individual words and punctuation symbols\n",
    "   * Remove stop words and filter tokens\n",
    "   * **POS (part of speech) Tagging**  \n",
    "   * **Normalization: Stemming, Lemmatization**\n",
    "   * **Named Entity Recognition (NER)**\n",
    "   * **Term Frequency and Inverse Dcoument Frequency (TF-IDF)**\n",
    "   * **Create document-to-term matrix (bag of words)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Sample text for analysis\n",
    "\n",
    "news=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]\n",
    "text=\". \".join(news).lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS (Part of Speech) Tagging\n",
    "\n",
    " - What is POS Tagging:\n",
    "   * The process of marking up a word in a text as corresponding to a particular part of speech (e.g. nouns, verbs, adjectives, adverbs etc.), based on both **its definition**, as well as its **context** — adjacent and related words in a phrase, sentence, or paragraph. \n",
    " - Why POS Tagging: \n",
    "   * **disambiguation**: A word may have different meanings. POS tag is a potential strong signal for word sense disambiguation. For example, \"I fish a fish\"\n",
    "   * **Phrase extraction**: Use POS rules to define accepted phrases (or information unit), or collocations for indexing and retrieval:\n",
    "     * Adj + Noun, e.g. nice house\n",
    "     * Verb + Noun, e.g. play football\n",
    "     * typical collocation patterns (https://nlp.stanford.edu/fsnlp/promo/colloc.pdf):\n",
    "       - Adj + Noun: e.g. linear function\n",
    "       - Noun + Noun: e.g. regression coefficient\n",
    "       - Adj + Adj + Noun: e.g. Gaussian random variable\n",
    "       - Noun + Adj + Noun: e.g. mean squared error\n",
    "       - Noun + Noun + Noun: e.g. class probability function\n",
    "       - Noun + Preposition + Noun: e.g. dregrees of freedom\n",
    "   * **Filter tokens**:  some POS have less importance in retrieval, e.g. stopwords such as ‘a’, ‘an’, ‘the’, and other glue words like 'in', 'on', 'of' etc.\n",
    "   * Find other forms of a word based on POS\n",
    "        * Noun: plural and singular\n",
    "        * Verb: past, present and future tense\n",
    "        * Adjective: positive, comparative, and superlative\n",
    " - List of Penn Treebank Tags can be found at https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    " - A tagger (program for tagging) is trained based on a corpus using machine learning approaches. It may not be very accurate when applying it your corpus.\n",
    "   - Stanford tagger (~97%)\n",
    "   - NLTK default tagger (PerceptronTagger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.1. To find all tags in treebank\n",
    "nltk.help.upenn_tagset()\n",
    "\n",
    "# find the meaning of a specific tag\n",
    "nltk.help.upenn_tagset('JJ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.2. NLTK POS Tagging\n",
    "\n",
    "# The input to the tagging function is a list of words\n",
    "\n",
    "# tokenize the text\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "# tag each tokenized word\n",
    "tagged_tokens= nltk.pos_tag(tokens)\n",
    "\n",
    "tagged_tokens\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.3. Extract Phrases by POS\n",
    "\n",
    "# Extract phrases in pattern of adjective + noun\n",
    "# i.e. nice house, growing market\n",
    "\n",
    "bigrams=list(nltk.bigrams(tagged_tokens))\n",
    "#print(bigrams)\n",
    "\n",
    "phrases=[ (x[0],y[0]) for (x,y) in bigrams \\\n",
    "         if x[1].startswith('JJ') \\\n",
    "         and y[1].startswith('NN')]\n",
    "\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4.4. Extract Noun+Verb, \n",
    "# i.e. prices soar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Normalization: Stemming & Lemmatization\n",
    " - What is normalization:\n",
    "   - Converts a list of words in **different surface forms** to a more **uniform form**, e.g.\n",
    "        * a word with different forms, e.g. organize, organizes, organized, and organizing\n",
    "        * families of derivationally related words with similar meanings, such as democracy, democratic, and democratization.\n",
    " - Why normalization\n",
    "   - **improve text matching**: in many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n",
    "   - reduce featue space generated from text\n",
    " - Stemming and lemmatization are two common techinques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Stemming \n",
    "\n",
    "* **Stemming**: reducing inflected (or sometimes derived) words to their **stem, base or root** form. \n",
    "   * For example, **crying** -> **cri**. \n",
    "   * Stemming may not generate a real word, but a root form. \n",
    "   * The stemming program is called stemmer. \n",
    "       * Famous stemers are Porter stemmer, Lancaster Stemmer, Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.1. Stermming Using Porter Stemmer\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "print(\"Stem of organizing/organized/organizes/organization\")\n",
    "print(porter_stemmer.stem('organizing'))\n",
    "print(porter_stemmer.stem('organized'))\n",
    "print(porter_stemmer.stem('organizes'))\n",
    "print(porter_stemmer.stem('organization'))\n",
    "\n",
    "print(\"\\nStem of crying\")\n",
    "print(porter_stemmer.stem('crying'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Lemmatization\n",
    "\n",
    "* **Lemmatization**: determining the lemma for a given word, \n",
    "   * A lemma is a word which stands at the head of a definition in a dictionary, e.g. run (lemma),  runs, ran and running (inflections) \n",
    "   * Lemmatization is a complex task involving understanding context and determining the part of speech of a word in a sentence \n",
    "      * e.g. \"organized\" (verb or adjective?)\n",
    "   * The widely used Lemmatization method is based on WordNet, a large lexical database of English.\n",
    "\n",
    "* **Difference** between stemming and lemmatization: \n",
    "   * a stemmer operates on a single word **without knowledge of the context**, and therefore cannot discriminate between words which have different meanings depending on part of speech. While, lemmatization **requires context and POS tags**. \n",
    "   * Stemming may not generate a real word, but lemmization always generates real words.\n",
    "   *  However, stemmers are typically easier to implement and run faster with reduced accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2.1. Lemmatization\n",
    "\n",
    "# wordnet lemmatizer takes POS tag as a parameter\n",
    "# However, wordnet has its own tag set, \n",
    "# different from treebank tag set\n",
    "# The default POS tag is noun \n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"organizing (verb)\", \\\n",
    "      wordnet_lemmatizer.lemmatize\\\n",
    "      ('organizing', wordnet.VERB))\n",
    "print('organized (verb)', \\\n",
    "      wordnet_lemmatizer.lemmatize\\\n",
    "      ('organized', wordnet.VERB))\n",
    "print('organized (adjective)',\\\n",
    "      wordnet_lemmatizer.lemmatize('organized', \\\n",
    "                                   wordnet.ADJ))\n",
    "print('organization (noun)',\\\n",
    "      wordnet_lemmatizer.lemmatize('organization'))\n",
    "print('crying (adjective)',\\\n",
    "      wordnet_lemmatizer.lemmatize('crying', \\\n",
    "                                   wordnet.ADJ))\n",
    "print('crying (verb)', \\\n",
    "      wordnet_lemmatizer.lemmatize('crying', \\\n",
    "                                   wordnet.VERB))\n",
    "\n",
    "# compare the result with Exercise 5.1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Named Entity Recognition (NER)\n",
    "\n",
    "- Definition: find and classify real word entities (Person, Organization, Event etc.) in text\n",
    "- Example: sentence \"Jim bought 300 shares of Acme Corp. in 2006\" can be annotated as \"**[Jim]<sub>Person</sub>** bought 300 shares of **[Acme Corp.]<sub>Organization</sub>** in 2006\"\n",
    "- Uses of NER:\n",
    "   *  Information Extraction: extract clear, factual information, i.e., Who did what to whom when?\n",
    "   *  Named entities can be indexed, and their relations can be extracted.\n",
    "   *  Sentiment can be attributed to companies or products\n",
    "   *  For question answering, answers are often named entities.\n",
    "- Techniques for NER\n",
    "   * Regular expression: Telephone numbers, emails, Capital names (e.g. Capitalized word + {city,  center, river}\n",
    "      * Adantages: simple and sometimes effective\n",
    "      * Disadvantage: \n",
    "         * first word of a sentence is capitalized; sometimes, titles are all capitalized; new proper names constantly emerges (e.g. movie titles, books, etc.)\n",
    "         * proper names may be ambiguous, e.g. Jordan can be *person* or *location*\n",
    "   * Supervised learning (IOB) (https://arxiv.org/abs/cmp-lg/9505040)\n",
    "       1. Collect a set of representative training documents\n",
    "       2. Label each token for its entity class (I: inside entity, B: begining entity) or other (O)\n",
    "       3. Design feature extractors appropriate to the text and classes, e.g. current word, pre/next word, pos tags etc.\n",
    "       4. Train a sequence classifier to predict the labels from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 6.1. Use NLTK for Named Entity Recognition\n",
    "\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    " \n",
    "sentence = \"Jim bought 300 shares of Acme Corp. in 2006.\"\n",
    "\n",
    "# the input to ne_chunk is list of (token, pos tag) tuples\n",
    "ner_tree=ne_chunk(pos_tag(word_tokenize(sentence)))\n",
    "\n",
    "# ne_chunk returns a tree\n",
    "print(ner_tree)\n",
    "\n",
    "# get PERSON out of the tree\n",
    "person=[]\n",
    "for t in ner_tree.subtrees():\n",
    "    if t.label() == 'PERSON':\n",
    "        person.append(t.leaves())\n",
    "print(\"PERSON\",person)\n",
    "\n",
    "# how to extract organization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. NLP pipeline using spaCy\n",
    "\n",
    "- spaCy is a relatively new framework in the Python Natural Language Processing, but is getting popular\n",
    "- Provides models for Part Of Speech tagging, Named Entity Recognition and Dependency Parsing\n",
    "<img src='spacy_pipeline.png'>\n",
    "- Supports 8 languages out of the box\n",
    "- Provides easy and beautiful visualizations\n",
    "- Provides pretrained word vectors\n",
    "- installation:\n",
    "  1. pip install spacy\n",
    "  2. python -m spacy download en\n",
    "- For a nice tutorial, see https://nlpforhackers.io/complete-guide-to-spacy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 7.1. Load spacing model\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# if you downloaded en_core_web_sm use the following:\n",
    "#import en_core_web_sm \n",
    "#nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.2. parse text\n",
    "\n",
    "# Create document pipeline\n",
    "doc = nlp(\"Next week I'm going to Madrid.\")\n",
    "\n",
    "# print header\n",
    "print(\"Text\\tLemma\\tPunct?\\tSpace?\\tPOS\\tTag\")\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "# print text/lemma/... for each token\n",
    "for token in doc:\n",
    "    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{4}\\t{5}\".format(\n",
    "        token.text,         # original text\n",
    "        token.lemma_,       # lemma\n",
    "        token.is_punct,     # is it a punctuation ?\n",
    "        token.is_space,     # is it a space\n",
    "        token.pos_,         # The universal part-of-speech tag.\n",
    "        token.tag_          # The treebank part-of-speech tag\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 7.3. Entity Recognition\n",
    "\n",
    "doc = nlp(\"Jim bought 300 shares of Acme Corp. in 2006.\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Term Frequency and Inverse Dcoument Frequency (TF-IDF)\n",
    " - Motivation: How to identify important words (or phrases, named entities) in a text in a collecton or corpus? When search for documents, we'd like to have these important words are matched.\n",
    " - Intuition: \n",
    "   * In a document, if a word/term/phrase is repeated many times, it is likely important. \n",
    "   * However, if it appears in most of the documents in the corpus, then it has little discriminating power in determining relevance. \n",
    "   * For instance, a collection of documents on the auto industry is likely to have the term auto in almost every document. Search by \"auto\" you may get all the documents. \n",
    " - ** TF-IDF**: is composed by two terms: \n",
    "      - **TF(Term Frequency)**: which measures how frequently a term, say w, occurs in a document. \n",
    "      - **IDF (Inverse Document Frequency)**: measures how important a term is within the corpus. \n",
    " \n",
    " - TF-IDF provides another way to remove stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Term Frequency (TF)\n",
    "- Measures how frequently a term, say w, occurs in a document, say $d$. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. \n",
    "- Thus, the frequency of $w$ in $d$, denoted as $freq(w,d)$ is often divided by the document length (a.k.a. the total number of terms in the document, denoted as $|d|$) as a way of normalization: $$tf(w,d) = \\frac{freq(w,d)}{|d|}$$\n",
    "- Example: d=\"Stocks end up near year end\"\n",
    "   * tf('Stocks',d)=?\n",
    "   * tf('end',d)=?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Inverse Document Frequency (IDF)\n",
    "- Measures how important a term is within the corpus. \n",
    "- However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. \n",
    "- Thus we need to weigh down the frequent terms while scale up the rare ones. \n",
    "- Let $|D|$ denote the number of documents, $df(w,D)$ denotes the number of documents with term $w$ in them. Then, $$idf(w) = ln(\\frac{|D|}{df(w,D)})+1$$ Or a smoothed version: $$idf(w) = ln(\\frac{|D|+1}{df(w,D)+1})+1$$\n",
    "- Examples: \n",
    "  * Considering dataset:\n",
    "       1. \"Oil prices soar to all-time record\", \n",
    "       2. \"Stocks end up near year end\", \n",
    "       3. \"Money funds rose in latest week\",\n",
    "       4. \"Stocks up; traders eye crude oil prices\",\n",
    "       5. \"Dollar rising broadly on record trade gain\"\n",
    "  * idf('Stocks')=?\n",
    "  * idf('all-time')=?\n",
    "  * Discussion:\n",
    "     * What words get very low IDF score?\n",
    "     * What words get very high IDF score?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. TF-IDF \n",
    "- Let $s(w,d)=tf(w,d) * idf(w)$, normalize the TF-IDF score of each word in a document normalized by the Euclidean norm, then \n",
    "   $$tfidf(w,d)=\\frac{s(w,d)}{\\sqrt{\\sum_{w \\in d}{s(w,d)^2}}}$$\n",
    "- For details of Euclidean norm (a.k.a L2 norm), see https://stanford.edu/class/ee103/lectures/norm/norm_slides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 8.1. computing tf-idf\n",
    "\n",
    "\n",
    "import nltk, re, string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# library for normalization\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# numpy is the package for matrix caculation\n",
    "import numpy as np  \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "docs=[\"Oil prices soar to all-time record\", \n",
    "\"Stocks end up near year end\", \n",
    "\"Money funds rose in latest week\",\n",
    "\"Stocks up; traders eye crude oil prices\",\n",
    "\"Dollar rising broadly on record trade gain\"]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1. get tokens of each document as list\n",
    "\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, or lemmatization here\n",
    "    \n",
    "    # create token count dictionary\n",
    "    token_count=nltk.FreqDist(tokens)\n",
    "    \n",
    "    # or you can create dictionary by yourself\n",
    "    #token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "# step 2. process all documents to \n",
    "# a dictionary of dictionaries\n",
    "docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "print(docs_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3. get document-term matrix\n",
    "# contruct a document-term matrix where \n",
    "# each row is a doc \n",
    "# each column is a token\n",
    "# and the value is the frequency of the token\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# since we have a small corpus, we can use dataframe \n",
    "# to get document-term matrix\n",
    "# but don't use this when you have a large corpus\n",
    "\n",
    "dtm=pd.DataFrame.from_dict(docs_tokens, \\\n",
    "                           orient=\"index\" )\n",
    "dtm=dtm.fillna(0)\n",
    "dtm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 4. get normalized term frequency (tf) matrix\n",
    "\n",
    "# convert dtm to numpy arrays\n",
    "tf=dtm.values\n",
    "\n",
    "# sum the value of each row\n",
    "doc_len=tf.sum(axis=1)\n",
    "print(doc_len)\n",
    "\n",
    "# divide dtm matrix by the doc length matrix\n",
    "tf=np.divide(tf, doc_len[:,None])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 5. get idf\n",
    "\n",
    "# get document freqent\n",
    "df=np.where(tf>0,1,0)\n",
    "#df\n",
    "\n",
    "# get idf\n",
    "idf=np.log(np.divide(len(docs), \\\n",
    "        np.sum(df, axis=0)))+1\n",
    "print(\"\\nIDF Matrix\")\n",
    "print (idf)\n",
    "\n",
    "\n",
    "smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    "print(\"\\nSmoothed IDF Matrix\")\n",
    "print(smoothed_idf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 6. get tf-idf\n",
    "print(\"TF-IDF Matrix\")\n",
    "tf_idf=normalize(tf*idf)\n",
    "print(tf_idf)\n",
    "\n",
    "print(\"\\nSmoothed TF-IDF Matrix\")\n",
    "smoothed_tf_idf=normalize(tf*smoothed_idf)\n",
    "print(smoothed_tf_idf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF matrix gives **weight** of each work in each document\n",
    "- Documents:\n",
    "    1. \"Oil prices soar to all-time record\", \n",
    "    2. \"Stocks end up near year end\", \n",
    "    3. \"Money funds rose in latest week\",\n",
    "    4. \"Stocks up; traders eye crude oil prices\",\n",
    "    5. \"Dollar rising broadly on record trade gain\"\n",
    "<img src='tfidf.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 8.2. Find the top three words \n",
    "# of each document by TF-IDF weight\n",
    "\n",
    "top=smoothed_tf_idf.argsort()[:,::-1][:,0:3]\n",
    "print(top)\n",
    "for row in top:\n",
    "    print([dtm.columns[x] for x in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. What to do with TF-IDF\n",
    "- This is the feature sapce of text mining (**Bag of Words**, **Vector Space Model**)\n",
    "- Identify important words in each document\n",
    "- Find similar documents\n",
    "    * How to measure simialrity (or distance)? http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.332.4480&rep=rep1&type=pdf\n",
    "    * Euclidean distance: Euclidean distance is **large** for vectors of high dimension (curse of dimensionality)\n",
    "    * Cosine similarity: The similarity between two documents is a function of the angle between their vectors in the if-idf vector space. \n",
    "      <img src='cosine.png' width=50% />\n",
    "      <img src='cosine_formula.svg' width=50% />\n",
    "      - Example: A=[0,2,1], B=[1,1,2], then\n",
    "      $$cosine(A,B)=\\frac{0*1+2*1+1*2}{\\sqrt{0+4+1}*\\sqrt{1+1+4}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Exercise 8.4.1 Document similarity\n",
    "\n",
    "# package to calculate distance\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# calculate cosince distance of every pair of documents \n",
    "# convert the distance object into a square matrix form\n",
    "# similarity is 1-distance\n",
    "similarity=1-distance.squareform\\\n",
    "(distance.pdist(tf_idf, 'cosine'))\n",
    "print(similarity)\n",
    "\n",
    "# find top doc similar to first one\n",
    "print(np.argsort(similarity)[:,::-1][0,0:2])\n",
    "\n",
    "for idx, doc in enumerate(docs):\n",
    "    print(idx,doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5. Put Everyting together -- Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords\n",
    "# numpy is the package for matrix cacluation\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "# Step 1. get tokens of each document as list\n",
    "def get_doc_tokens(doc):\n",
    "    tokens=[token.strip() \\\n",
    "            for token in nltk.word_tokenize(doc.lower()) \\\n",
    "            if token.strip() not in stop_words and\\\n",
    "               token.strip() not in string.punctuation]\n",
    "    \n",
    "    # you can add bigrams, collocations, stemming, \n",
    "    # or lemmatization here\n",
    "    \n",
    "    token_count={token:tokens.count(token) for token in set(tokens)}\n",
    "    return token_count\n",
    "\n",
    "def tfidf(docs):\n",
    "    # step 2. process all documents to get list of token list\n",
    "    docs_tokens={idx:get_doc_tokens(doc) \\\n",
    "             for idx,doc in enumerate(docs)}\n",
    "\n",
    "    # step 3. get document-term matrix\n",
    "    dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "    dtm=dtm.fillna(0)\n",
    "      \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "    tf=dtm.values\n",
    "    doc_len=tf.sum(axis=1)\n",
    "    tf=np.divide(tf.T, doc_len).T\n",
    "    \n",
    "    # step 5. get idf\n",
    "    df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "    smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "    smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. gensim package\n",
    "- Gensim is an open source Python library for NLP, with a focus on topic modeling.\n",
    "- It is not an everything-including-the-kitchen-sink NLP research library (like NLTK); instead, Gensim is a mature, focused, and efficient suite of NLP tools for topic modeling, including \n",
    "  - Word2Vec word embedding \n",
    "  - Topic modeling\n",
    "  - Text preprocessing like **phrase extraction**\n",
    "  \n",
    "- Gensim Phrase Model: \n",
    "    - **gensim.models.phrases.Phrases(sentences, min_count, threshold, max_vocab_size, delimiter, scoring, ...)**\n",
    "        - *sentences*: list of sentences or iterables, each of which can be a document\n",
    "        - *min_count*: Ignore all words and bigrams with total collected count lower than this value.\n",
    "        - *threshold*: Represent a score threshold for forming the phrases (higher means fewer phrases). A phrase of words $a$ followed by $b$ is accepted if the score of the phrase is greater than threshold. Heavily depends on concrete scoring-function.\n",
    "        - *max_vocab_size*: Maximum size (number of tokens) of the vocabulary. \n",
    "        - *delimiter*: Glue character used to join collocation tokens, should be a byte string (e.g. '\\_').\n",
    "        - *scoring*: Specify how potential phrases are scored. \n",
    "           - **default** - original_scorer(), by Mikolov et al. (2013) (https://arxiv.org/pdf/1310.4546.pdf)\n",
    "           - **npmi** - npmi_scorer().\n",
    "- For details, check - https://radimrehurek.com/gensim/models/phrases.html           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9.1. Find bigrams using gensim\n",
    "\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# load a built-in NLTK corpus as a list of words\n",
    "words=nltk.corpus.gutenberg.words('austen-sense.txt')\n",
    "\n",
    "# Train phrase model to find phrases using original_scorer\n",
    "phrases = Phrases([words], min_count=5, threshold=10)\n",
    "\n",
    "for phrase, score in phrases.export_phrases([words]):\n",
    "    print(phrase, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9.2. Find bigrams by NPMI\n",
    "\n",
    "# find phrases using NPMI\n",
    "\n",
    "phrases = Phrases([words], min_count=5, threshold=0.4, \\\n",
    "                  scoring='npmi')\n",
    "\n",
    "for phrase, score in phrases.export_phrases([words]):\n",
    "    print(phrase, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 9.3. Tokenize by unigrams and bigrams\n",
    "\n",
    "# Initialize phrase tokenizer\n",
    "bigram = Phraser(phrases)\n",
    "\n",
    "sent=\"As dinner was not to be ready in less than two hours from their arrival,\"\n",
    "print(bigram[nltk.word_tokenize(sent.lower())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
