{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Unsupervised Learing: Text Clustering</center>\n",
    "\n",
    "References:\n",
    "* https://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf\n",
    "* http://infolab.stanford.edu/~ullman/mmds/ch7.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Clustering vs. Classification\n",
    "* Clustering (Unsupervised): divide a set of objects into clusters (parts of the set) so that objects in the same cluster are similar to each other, and/or objects in different clusters are dissimilar.\n",
    "  * Representation of the objects\n",
    "  * Similarity/distance measure\n",
    "* Classifification (Supervised): group objects into predetermined categories\n",
    "  * Representation of the objects\n",
    "  * A training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why clustering\n",
    "* Understand conceptually meaningful groups of objects that share common characteristics \n",
    "* Provides an abstraction from individual data objects to the clusters in which those data objects reside\n",
    "* Uses of clustering\n",
    "  * Summarization\n",
    "  * Compression\n",
    "  * Efficiently finding nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Types of Clusterings \n",
    "* Different kinds of models (https://www.geeksforgeeks.org/different-types-clustering-algorithm/):\n",
    "  - Centroid models (partition): \n",
    "     - Similarity is derived as the closeness of a data point to the centroid of clusters. \n",
    "     - Flat partition, e.g. K-Means\n",
    "     <img src='centroid.png' width=\"40%\">\n",
    "  - Connectivity models (Hierarchical algorithms): \n",
    "     - Data points closer in data space exhibit more similarity to each other than the data points lying farther away.      \n",
    "     - Hierarchy of clusters, e.g. agglomerative clustering\n",
    "     <img src='connectivity.png' width=\"40%\">\n",
    "  - Distribution models: \n",
    "     - How probable is it that all data points in the cluster belong to the same distribution, concept, or topic\n",
    "     - e.g. Latent Semantics Analysis, Latent Dirichlet Allocation (LDA)\n",
    "     <img src='distribution.png' width=\"40%\">\n",
    "  - Density models: clusters correspond to areas of varied density of data points in the data space\n",
    "     - e.g. DBSCAN\n",
    "     <img src='density.png' width=\"40%\">\n",
    "* Exclusive vs. Overlapping\n",
    "  - Exclusive: each object is assigned to a single cluster, e.g. K-Means\n",
    "  - Overlapping (non-exclusive): an object can simultaneously belong to more than one cluster, e.g. LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluation of Clustering: What is a good clustering\n",
    "### 4.1 External Evaluation: \n",
    "* External evaluation measures the degree to which predicted clustering labels correspond to actual class labels  \n",
    "* **Precision** and **Recall**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Internal Evaluation \n",
    "<img src='cohension_separation.png' width=\"60%\">\n",
    "* **Cohension (Intra-cluster similarity)**: how \"cohesive\" a cluster is, i.e. the average similarity of objects in the same cluster. \n",
    "   - e.g. cluster radius: $\\max{d(x, μ_A)}$ where $μ_A$ is the arithmetic mean of cluster A and $x$ is a point in A\n",
    "   - e.g. cluster diameter: $\\max{d(x, y)}$ where $x,y$ are two points in cluster A\n",
    "\n",
    "* **Separation (Inter-cluster dissimilarity)**: how \"separate\" a cluster from another, i.e. the average similarity of all samples in cluster $A$ to all the samples in cluster $B$.\n",
    "   - e.g. Separation can be calculated as average distance: $\\frac{1}{|A|*|B|}\\sum_{x \\in A}{\\sum_{y \\in B}{d(x, y)}}$ \n",
    "* Metrics with combined cohension and separation (http://scikit-learn.org/stable/modules/clustering.html#clustering-evaluation)\n",
    "   - Silhouette Coefficient: $s=\\frac{b-a}{\\max(a,b)}$, where $a$: the mean distance between a sample and all other points in the same cluster, and $b$: the mean distance between a sample and all other points in the next nearest cluster. $s \\in [-1, 1]$.\n",
    "   - Calinski-Harabaz Index: $s=\\frac{b}{a}$ where $a$ is mean within\\-cluster separation, and $b$ is the mean between\\-cluster separation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K-Means\n",
    "### 5.1. Algorithm outline: Cluster objects into K clusters\n",
    "<img style=\"float: left;\" src='Kmean1.png'  width='20%'/><img  src='Kmean2.png' width='20%'/>\n",
    "- Algorithm: \n",
    "    1. Select K points as initial centroids \n",
    "    2. Repeat until centroids do not change:\n",
    "        1. Form K clusters by assigning each point to its closest centroid by distance.\n",
    "        2. Recompute the centroid of each cluster as the arithmetic mean of samples within the cluster. \n",
    "- A few observations of K-means:\n",
    "  - Initial centroids have an impact on clustering. Usually, several rounds of clustering with random initial centroids are performed, and the most commonly occurring output centroids are chosen.\n",
    "  - Centroids and distance measure are crtical in the algorithm\n",
    "     - **Euclidean distance**: \n",
    "       - The best centroid for minimizing the average distance from all samples to the centroid is the mean of points in the cluster (https://www-users.cs.umn.edu/~kumar001/dmbook/ch8.pdf)\n",
    "       - Curse of dimensionality\n",
    "       - Sensitive to outliers\n",
    "     - **Cosine similarity**: \n",
    "       * Well-accepted similarity measure for documents\n",
    "       * It is not guaranteed that the mean of samples in a cluster is the best centroid \n",
    "       * For text clustering, the centroid does not stand for an actual document. How to interpret clusters?\n",
    "     - A modified version of Kmeans is called **K-medoids**, where a representative sample is choosen as the center of a cluster, called as a medoid. \n",
    "- Python packages for Kmean\n",
    "  * NLTK: can choose Euclidean or Cosine similarity as distance measure\n",
    "  * Sklearn: only Euclidean distance is supported  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.1 Load data and generate TF-IDF\n",
    "# Load datasets (http://qwone.com/~jason/20Newsgroups/)\n",
    "# A subset is loaded\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df=pd.read_csv(\"../../dataset/twenty_news_data.csv\",\\\n",
    "               header=0)\n",
    "\n",
    "# Select three labels for now\n",
    "labels =['comp.graphics', 'soc.religion.christian',\\\n",
    "         'sci.med']\n",
    "data=df[df[\"label\"].isin(labels)]\n",
    "\n",
    "# Split dataset into training and test. \n",
    "# Assuming we only know ground-truth label \n",
    "# for the test set.\n",
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=0)\n",
    "\n",
    "# print out the full text of the first sample\n",
    "print(data[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.2\n",
    "# initialize the TfidfVectorizer \n",
    "# set min document frequency to 5\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# set the min document frequency to 5\n",
    "# generate tfidf matrix\n",
    "tfidf_vect = TfidfVectorizer(stop_words=\"english\",\\\n",
    "                             min_df=5) \n",
    "\n",
    "dtm= tfidf_vect.fit_transform(train[\"text\"])\n",
    "print (dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.3 Clustering using NLTK KMean\n",
    "# cosine distance is calculated\n",
    "\n",
    "from nltk.cluster import KMeansClusterer, \\\n",
    "cosine_distance\n",
    "\n",
    "# set number of clusters\n",
    "num_clusters=3\n",
    "\n",
    "# initialize clustering model\n",
    "# using cosine distance\n",
    "# clustering will repeat 20 times\n",
    "# each with different initial centroids\n",
    "clusterer = KMeansClusterer(num_clusters, \\\n",
    "                            cosine_distance, \\\n",
    "                            repeats=20)\n",
    "\n",
    "# samples are assigned to cluster labels \n",
    "# starting from 0\n",
    "clusters = clusterer.cluster(dtm.toarray(), \\\n",
    "                             assign_clusters=True)\n",
    "\n",
    "#print the cluster labels of the first 5 samples\n",
    "print(clusters[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.1.4 Interpret each cluster by centroid\n",
    "\n",
    "# a centroid is the arithemtic mean \n",
    "# of all samples in the cluster\n",
    "# it may not stand for a real document\n",
    "\n",
    "# find top words at centroid of each cluster\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "# clusterer.means() contains the centroids\n",
    "# each row is a cluster, and \n",
    "# each column is a feature (word)\n",
    "centroids=np.array(clusterer.means())\n",
    "\n",
    "# argsort sort the matrix in ascending order \n",
    "# and return locations of features before sorting\n",
    "# [:,::-1] reverse the order\n",
    "sorted_centroids = centroids.argsort()[:, ::-1] \n",
    "\n",
    "# The mapping between feature (word)\n",
    "# index and feature (word) can be obtained by\n",
    "# the vectorizer's function get_feature_names()\n",
    "voc_lookup= tfidf_vect.get_feature_names()\n",
    "\n",
    "for i in range(num_clusters):\n",
    "    \n",
    "    # get words with top 20 tf-idf weight in the centroid\n",
    "    top_words=[voc_lookup[word_index] \\\n",
    "               for word_index in sorted_centroids[i, :20]]\n",
    "    print(\"Cluster %d:\\n %s \" % (i, \"; \".join(top_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. How to evaluate clustering\n",
    "- External evaluation:\n",
    "  - Obtain \"ground truth\": if data is not labeled, manually label a random subset of samples as \"ground truth\" \n",
    "  - Assign each cluster to a \"true\" class by the **majority vote rule**\n",
    "  - Calculate precision and recall\n",
    "  \n",
    "  \n",
    "  | Cluster ID      | Ground Truth Class Label   |\n",
    "  | :------------- |:----------------------------|\n",
    "  | 0      | comp.graphics|\n",
    "  | 1      | sci.med  |\n",
    "  | 2      | soc.religion.christian|\n",
    "  \n",
    "- Internal evaluation\n",
    "  - Silhouette Coefficient\n",
    "  - Calinski-Harabaz Index\n",
    "  - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2.1 Predict labels for new samples\n",
    "\n",
    "# Question: how to determine \n",
    "# the label for a new sample?\n",
    "\n",
    "# note transform function is used\n",
    "# not fit_transform\n",
    "test_dtm = tfidf_vect.transform(test[\"text\"])\n",
    "\n",
    "predicted = [clusterer.classify(v) for v in test_dtm.toarray()]\n",
    "\n",
    "predicted[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.2.2 External evaluation\n",
    "# determine cluster labels and calcuate precision and recall\n",
    "\n",
    "# Create a dataframe with cluster id and \n",
    "# ground truth label\n",
    "confusion_df = pd.DataFrame(list(zip(test[\"label\"].values, predicted)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "confusion_df.head()\n",
    "\n",
    "# generate crosstab between clusters and true labels\n",
    "pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.2.3 \n",
    "# Map cluster id to true labels by \"majority vote\"\n",
    "cluster_dict={0:'comp.graphics',\\\n",
    "              1:\"sci.med\",\\\n",
    "              2:'soc.religion.christian'}\n",
    "\n",
    "# Map true label to cluster id\n",
    "predicted_target=[cluster_dict[i] \\\n",
    "                  for i in predicted]\n",
    "\n",
    "print(metrics.classification_report\\\n",
    "      (test[\"label\"], predicted_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Clustering with sklearn package - Euclidean distance\n",
    "- Compare its performance with NLTK Kmeans result\n",
    "- Discuss: the difference between performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Exercise 5.3.1 Clustering with sklearn package - Euclidean distance\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Kmeans with 20 different centroid seeds\n",
    "km = KMeans(n_clusters=num_clusters, n_init=20)\\\n",
    ".fit(dtm)\n",
    "clusters = km.labels_.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 5.3.2 Performance Evaluation\n",
    "\n",
    "predicted = km.predict(test_dtm)\n",
    "\n",
    "confusion_df = pd.DataFrame(list(zip(test[\"label\"].values, predicted)),\\\n",
    "                            columns = [\"label\", \"cluster\"])\n",
    "confusion_df.head()\n",
    "\n",
    "# generate crosstab between clusters and true labels\n",
    "pd.crosstab( index=confusion_df.cluster, columns=confusion_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3.3 Change the mapping accordingly\n",
    "cluster_dict={1:'comp.graphics', 0:\"sci.med\",\\\n",
    "              2:'soc.religion.christian'}\n",
    "\n",
    "# Map true label to cluster id\n",
    "predicted_target=[cluster_dict[i] \\\n",
    "                  for i in predicted]\n",
    "\n",
    "print(metrics.classification_report\\\n",
    "      (test[\"label\"], predicted_target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You may notice the significant performance difference caused by:\n",
    "   - different distance measures: L2 distance vs. Cosine distance\n",
    "   - high dimensionalties (curse of dimensionality)\n",
    "- What could be possible ways to solve \"curse of dimensionality\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. How to pick *K*, the number of clusters?\n",
    "- **Try external valuation first!!!**\n",
    "  - manually assess a subset of documents to create \"ground truth\"\n",
    "- In case it is impossible to figure out how many clusters in the data set manually, **theorectically**, *K* may be selected as follows:\n",
    "  * Select a metric to measure the \"goodness\" of clusters, e.g. average radius, average diameter, etc.\n",
    "  * Varying *K* from 2 to N, perform clustering for each *K*\n",
    "  * Ideally, as *K* increases to some point, the metric should grow slowly (**elbow method**)\n",
    "\n",
    "<img style=\"float: left;\" src='best_k.png'  width='40%'/><img  src='sample1.png' width='30%'/>\n",
    "source: http://infolab.stanford.edu/~ullman/mmds/ch7.pdf\n",
    "- However, if samples do not have clear structures, this method may not work (elbow does not exist!)\n",
    "<img src=\"samples2.png\" width='30%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
